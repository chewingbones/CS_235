{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3a12c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages and data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import random\n",
    "\n",
    "random.seed(1337)\n",
    "df = pd.read_csv(\"winequality-red.csv\", delimiter = ';')\n",
    "\n",
    "def qclass(x):\n",
    "    if x >= 7:\n",
    "        return 'above_avg'\n",
    "    else:\n",
    "        return 'avg_or_less'\n",
    "\n",
    "df['quality_class'] = df['quality'].apply(qclass)\n",
    "\n",
    "df = df.drop('quality', axis=1)\n",
    "X = df.drop('quality_class', axis=1)\n",
    "y = df['quality_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "429c72d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#func to partition data at a given boundary\n",
    "def partition(data, feature, boundary):\n",
    "\n",
    "    true_rows = data[data[feature] >= boundary]\n",
    "    false_rows = data[data[feature] < boundary]\n",
    "            \n",
    "    return true_rows, false_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aa7da73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the gini impurity for calculating the impurity of a node\n",
    "def gini(data, target):\n",
    "\n",
    "    counts = data[target].value_counts()\n",
    "    impurity = 1\n",
    "    \n",
    "    for c in counts:\n",
    "        prob_of_label = c/float(len(data))\n",
    "        impurity -= prob_of_label**2\n",
    "        \n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b385cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the function to calculate information gain from splitting a node\n",
    "def info_gain(left, right, current_uncertainty, target):\n",
    "\n",
    "    p = float(len(left))/(len(left)+len(right))\n",
    "    split_uncertainty = p*gini(left, target) + (1-p)*gini(right, target)\n",
    "    ig = current_uncertainty - split_uncertainty\n",
    "    \n",
    "    return ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "339c9773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick the best feature and boundary combination that produces the highest information gain \n",
    "def best_split(data, target):\n",
    "\n",
    "    best_gain = 0\n",
    "    best_feature = None\n",
    "    best_boundary = None\n",
    "    current_uncertainty = gini(data, target)\n",
    "    features = data.columns.drop(target)\n",
    "\n",
    "    for f in features:\n",
    "        if best_feature == None:\n",
    "            best_feature = f\n",
    "            \n",
    "        boundaries = list(set(data[f]))\n",
    "\n",
    "        for b in boundaries:\n",
    "            if best_boundary == None:\n",
    "                best_boundary = b\n",
    "                \n",
    "            tr, fr = partition(data, f, b)\n",
    "            \n",
    "            if (len(tr) == 0) or (len(fr) == 0):\n",
    "                continue\n",
    "\n",
    "            gain = info_gain(tr, fr, current_uncertainty, target)\n",
    "\n",
    "            if gain >= best_gain:\n",
    "                best_gain, best_feature, best_boundary = gain, f, b\n",
    "\n",
    "    return best_gain, best_feature, best_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a605c334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create leaf nodes or branch nodes based on a number of conditions for the current data\n",
    "def make_node(data, target, split_level, min_leaf, max_depth):\n",
    "    \n",
    "    gain, feature, boundary = best_split(data, target)\n",
    "    node = {\n",
    "            'info_gain': gain, \n",
    "            'size': len(data), \n",
    "            'feature': feature, \n",
    "            'boundary': boundary\n",
    "           }\n",
    "    \n",
    "    if (gain == 0) or (len(data) <= min_leaf) or (split_level > max_depth):\n",
    "        node['node_type'] = 'leaf'\n",
    "        prediction = {}\n",
    "        counts = data[target].value_counts()\n",
    "        prediction = 0\n",
    "        prediction_chance = 0\n",
    "        for i in counts.index:\n",
    "            prediction_chance_i = counts[i]/float(len(data))\n",
    "            if prediction_chance_i > prediction_chance:\n",
    "                prediction_chance = prediction_chance_i\n",
    "                prediction = i\n",
    "        node['prediction'] = prediction\n",
    "        node['prediction_chance'] = prediction_chance\n",
    "        \n",
    "    elif gain > 0:\n",
    "        node['node_type'] = 'branch'\n",
    "        \n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a6e44be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the performance metrics of a model for a given class\n",
    "def get_metrics(pred, target='quality_class', positive_class='above_avg'):\n",
    "    \n",
    "    correct_preds = pred[pred[target]==pred['predictions']]\n",
    "    correct = len(correct_preds)\n",
    "    correct_pos_preds = correct_preds[correct_preds[target]==positive_class]\n",
    "    correct_positive = len(correct_pos_preds)\n",
    "    positive_actual = len(pred[pred[target]==positive_class])\n",
    "    positive_pred = len(pred[pred['predictions']==positive_class])\n",
    "    total = len(pred)\n",
    "    acc = correct/total\n",
    "    precision = correct_positive/positive_pred\n",
    "    recall = correct_positive/positive_actual\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "    print('accuracy:',round(acc, 4),', precision:',round(precision, 4),', recall:',round(recall, 4),', f1:',round(f1, 4))\n",
    "    \n",
    "    return acc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1deb1a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the weighted performance metrics of a model\n",
    "def get_weighted_metrics(pred, target='quality_class'):\n",
    "    \n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    f1 = {}\n",
    "    class_n = {}\n",
    "    total = len(pred)\n",
    "    \n",
    "    classes = list(set(pred[target]))\n",
    "    for c in classes:\n",
    "        correct_preds = pred[pred[target]==pred['predictions']]\n",
    "        correct = len(correct_preds)\n",
    "        correct_pos_preds = correct_preds[correct_preds[target]==c]\n",
    "        correct_positive = len(correct_pos_preds)\n",
    "        positive_actual = len(pred[pred[target]==c])\n",
    "        positive_pred = len(pred[pred['predictions']==c])\n",
    "        acc = correct/total\n",
    "        precision[c] = correct_positive/positive_pred\n",
    "        recall[c] = correct_positive/positive_actual\n",
    "        f1[c] = 2*precision[c]*recall[c]/(precision[c]+recall[c])\n",
    "        class_n[c] = positive_actual\n",
    "    \n",
    "    w_precision = 0\n",
    "    w_recall = 0\n",
    "    w_f1 = 0\n",
    "    for c in classes:\n",
    "        w_precision+=(precision[c]*class_n[c]/total)\n",
    "        w_recall+=(recall[c]*class_n[c]/total)\n",
    "        w_f1+=(f1[c]*class_n[c]/total)\n",
    "        \n",
    "    print('accuracy:',round(acc, 4),', wprecision:',round(w_precision, 4),', wrecall:',round(w_recall, 4),', wf1:',round(w_f1, 4))\n",
    "    \n",
    "    return acc, w_precision, w_recall, w_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e074f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is where the magic happens! func to train the decision tree\n",
    "def dtree(data = {}, decision_tree = {}, \n",
    "          split_level = 0, max_depth = 7, min_leaf = 5, \n",
    "          target = 'quality_class'):\n",
    "    \n",
    "    if split_level == 0:\n",
    "        root = make_node(data, target, split_level, min_leaf, max_depth)\n",
    "        root['node_type'] = 'root'\n",
    "        decision_tree[split_level] = [(data, root)]\n",
    "        \n",
    "    decision_tree[split_level + 1] = []\n",
    "    keep_splitting = 0\n",
    "    \n",
    "    \n",
    "    for d, n in decision_tree[split_level]:\n",
    "        \n",
    "        if (n['node_type'] != 'leaf'):\n",
    "            \n",
    "            gain, feature, boundary = best_split(d, target)\n",
    "            tr, fr = partition(d, feature, boundary)\n",
    "\n",
    "            right_branch = make_node(tr, target, split_level + 1, min_leaf, max_depth)\n",
    "            n['right_child_feature'] = right_branch['feature']\n",
    "            n['right_child_boundary'] = right_branch['boundary']\n",
    "            decision_tree[split_level + 1].append((tr, right_branch))\n",
    "            if (right_branch['node_type'] == 'branch'):\n",
    "                keep_splitting = 1\n",
    "\n",
    "            left_branch = make_node(fr, target, split_level + 1, min_leaf, max_depth)\n",
    "            n['left_child_feature'] = left_branch['feature']\n",
    "            n['left_child_boundary'] = left_branch['boundary']\n",
    "            decision_tree[split_level + 1].append((fr, left_branch))\n",
    "            if (left_branch['node_type'] == 'branch'):\n",
    "                keep_splitting = 1\n",
    "    \n",
    "    split_level += 1\n",
    "    \n",
    "    if (keep_splitting == 1):\n",
    "        decision_tree, split_level = dtree(decision_tree = decision_tree, split_level = split_level)\n",
    "        \n",
    "    return decision_tree, split_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "863de709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#func to make predictions based on a trained tree\n",
    "def dtree_predict(data, tree, target = 'quality_class'):\n",
    "\n",
    "    predictions = {}\n",
    "            \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        x = data.iloc[i]\n",
    "        \n",
    "        for s in range(len(tree)):\n",
    "            \n",
    "            if (s == 0):\n",
    "                node = tree[s][0][1]\n",
    "                current_feature = node['feature']\n",
    "                current_boundary = node['boundary']\n",
    "                \n",
    "            for n in range(len(tree[s])):\n",
    "                \n",
    "                node = tree[s][n][1]\n",
    "                \n",
    "                if ((node['feature'] == current_feature) and (node['boundary'] == current_boundary)):\n",
    "                    \n",
    "                    if (node['node_type']!='leaf'):  \n",
    "                        \n",
    "                        if (x[current_feature] >= current_boundary):\n",
    "                            next_feature = node['right_child_feature']\n",
    "                            next_boundary = node['right_child_boundary']\n",
    "\n",
    "                        elif (x[current_feature] < current_boundary):\n",
    "                            next_feature = node['left_child_feature']\n",
    "                            next_boundary = node['left_child_boundary']\n",
    "                            \n",
    "                    if (node['node_type']=='leaf'):\n",
    "                        prediction = node['prediction']\n",
    "        \n",
    "            current_feature = next_feature\n",
    "            current_boundary = next_boundary\n",
    "        \n",
    "        predictions[i] = prediction\n",
    "        \n",
    "    data['predictions'] = pd.Series(predictions)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f978d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest func built upon the previous decision tree func\n",
    "def rforest_predict(test_data, train_data, target = 'quality_class', n_trees = 69):\n",
    "\n",
    "    samples = []\n",
    "    trees = []\n",
    "    dt_predictions = pd.DataFrame()\n",
    "    rf_predictions = []\n",
    "    classes = list(set(test_data[target]))\n",
    "\n",
    "    for i in range(n_trees):\n",
    "        sample_size = int(len(train_data)/2)\n",
    "        sample = train_data.sample(n=sample_size)\n",
    "        sample = sample.reset_index(drop=True)\n",
    "        samples.append(sample)\n",
    "\n",
    "    for s in samples:\n",
    "        tree, sl = dtree(s, target=target)\n",
    "        print('tree completed')\n",
    "        trees.append(tree)\n",
    "\n",
    "    pred_number = 1\n",
    "    for t in trees:\n",
    "        prediction = dtree_predict(test_data, t, target)\n",
    "        dt_predictions['prediction'+str(pred_number)] = prediction['predictions']\n",
    "        pred_number+=1\n",
    "\n",
    "    for i in range(len(dt_predictions)):\n",
    "        counts = {}\n",
    "\n",
    "        for c in classes:\n",
    "            counts[c] = 0\n",
    "\n",
    "        for col in dt_predictions.columns:\n",
    "            dt_pred = dt_predictions[col][i]\n",
    "            counts[dt_pred]+=1\n",
    "\n",
    "        biggest_count = 0\n",
    "        for c in counts.keys():\n",
    "            if counts[c] >= biggest_count:\n",
    "                biggest_count = counts[c]\n",
    "                rf_pred = c\n",
    "\n",
    "        rf_predictions.append(rf_pred)\n",
    "\n",
    "    test_data['rf_predictions'] = pd.Series(rf_predictions)  \n",
    "    \n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4e40361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a dict where keys are a fold and values are lists containing training and testing data,\n",
    "#training data for each fold has been undersampled to address class imbalance as well as reduce training runtime\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "skf.get_n_splits(X.copy(), y.copy())\n",
    "\n",
    "fold = 1\n",
    "skf_df ={}\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    \n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    xtr, ytr = rus.fit_resample(X.iloc[train_index], y.iloc[train_index])\n",
    "    train_df = pd.concat([xtr, ytr], axis=1).reset_index(drop=True)\n",
    "    test_df = pd.concat([X.iloc[test_index], y.iloc[test_index]], axis=1).reset_index(drop=True)\n",
    "    skf_df[fold] = [train_df, test_df]\n",
    "    fold+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08fc272c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "accuracy: 0.8063 , precision: 0.2727 , recall: 0.2857 , f1: 0.2791\n",
      "accuracy: 0.8063 , wprecision: 0.8101 , wrecall: 0.8063 , wf1: 0.8082\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "accuracy: 0.8187 , precision: 0.3824 , recall: 0.619 , f1: 0.4727\n",
      "accuracy: 0.8187 , wprecision: 0.8638 , wrecall: 0.8188 , wf1: 0.8357\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "accuracy: 0.475 , precision: 0.1961 , recall: 0.9091 , f1: 0.3226\n",
      "accuracy: 0.475 , wprecision: 0.8597 , wrecall: 0.475 , wf1: 0.5372\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "accuracy: 0.7562 , precision: 0.3023 , recall: 0.5909 , f1: 0.4\n",
      "accuracy: 0.7562 , wprecision: 0.8377 , wrecall: 0.7563 , wf1: 0.7856\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "accuracy: 0.7625 , precision: 0.34 , recall: 0.7727 , f1: 0.4722\n",
      "accuracy: 0.7625 , wprecision: 0.87 , wrecall: 0.7625 , wf1: 0.7953\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "accuracy: 0.7125 , precision: 0.2857 , recall: 0.7273 , f1: 0.4103\n",
      "accuracy: 0.7125 , wprecision: 0.852 , wrecall: 0.7125 , wf1: 0.755\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "accuracy: 0.6438 , precision: 0.2535 , recall: 0.8182 , f1: 0.3871\n",
      "accuracy: 0.6438 , wprecision: 0.8586 , wrecall: 0.6438 , wf1: 0.6992\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "accuracy: 0.7188 , precision: 0.129 , recall: 0.1818 , f1: 0.1509\n",
      "accuracy: 0.7188 , wprecision: 0.7599 , wrecall: 0.7188 , wf1: 0.7379\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "accuracy: 0.6875 , precision: 0.2586 , recall: 0.6818 , f1: 0.375\n",
      "accuracy: 0.6875 , wprecision: 0.8389 , wrecall: 0.6875 , wf1: 0.7344\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "tree completed\n",
      "accuracy: 0.6478 , precision: 0.2603 , recall: 0.9048 , f1: 0.4043\n",
      "accuracy: 0.6478 , wprecision: 0.8821 , wrecall: 0.6478 , wf1: 0.7043\n"
     ]
    }
   ],
   "source": [
    "#cross validate model and return performance metrics for each fold\n",
    "rf_cv_results_minority = []\n",
    "rf_cv_results_weighted = []\n",
    "rfpredslist = []\n",
    "\n",
    "for fold in skf_df:\n",
    "    kf_train_df = skf_df[fold][0]\n",
    "    kf_test_df = skf_df[fold][1]\n",
    "    \n",
    "    rfpreds = rforest_predict(test_data=kf_test_df, train_data=kf_train_df, target='quality_class', n_trees=11)\n",
    "    rfpredslist.append(rfpreds)\n",
    "    \n",
    "    fold_results_minority = [get_metrics(pred=rfpreds, target='quality_class', positive_class='above_avg')]\n",
    "    rf_cv_results_minority.append(fold_results_minority)\n",
    "    \n",
    "    fold_results_weighted = [get_weighted_metrics(pred=rfpreds, target='quality_class')]\n",
    "    rf_cv_results_weighted.append(fold_results_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44136863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy     0.702905\n",
      "precision    0.268065\n",
      "recall       0.649134\n",
      "f1           0.367415\n",
      "Name: mean_scores, dtype: float64\n",
      "accuracy     0.099752\n",
      "precision    0.070409\n",
      "recall       0.244655\n",
      "f1           0.096292\n",
      "Name: std_of_scores, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#find mean and std for each performance metric across k folds\n",
    "import numpy as np\n",
    "\n",
    "a = rf_cv_results_minority\n",
    "b = np.asarray(a)\n",
    "c = np.mean(b, axis=1)\n",
    "d = pd.DataFrame(c)\n",
    "e = d.rename(columns = {0:'accuracy', 1:'precision', 2:'recall', 3:'f1'})\n",
    "avg_metrics = e.mean(axis=0)\n",
    "avg_metrics.name = 'mean_scores'\n",
    "std_metrics = e.std(axis=0)\n",
    "std_metrics.name = 'std_of_scores'\n",
    "\n",
    "print(avg_metrics)\n",
    "print(std_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74034b92",
   "metadata": {},
   "source": [
    "##RESULTS OF PREVIOUS RUN##\n",
    "\n",
    "accuracy     0.684642\n",
    "precision    0.281667\n",
    "recall       0.659524\n",
    "f1           0.378611\n",
    "Name: mean_scores, dtype: float64\n",
    "\n",
    "accuracy     0.138242\n",
    "precision    0.112642\n",
    "recall       0.203107\n",
    "f1           0.120722\n",
    "Name: std_of_scores, dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6171e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[141  76]\n",
      " [399 983]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   above_avg       0.26      0.65      0.37       217\n",
      " avg_or_less       0.93      0.71      0.81      1382\n",
      "\n",
      "    accuracy                           0.70      1599\n",
      "   macro avg       0.59      0.68      0.59      1599\n",
      "weighted avg       0.84      0.70      0.75      1599\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix and classification report generated for reporting\n",
    "#also, prediction and target data are binarized in preparation for PR curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "bin_labels = {'above_avg': 1, 'avg_or_less': 0}\n",
    "\n",
    "allrfpred = pd.concat(rfpredslist, ignore_index=True)\n",
    "\n",
    "print(confusion_matrix(allrfpred['quality_class'], allrfpred['predictions']))\n",
    "print(classification_report(allrfpred['quality_class'], allrfpred['predictions']))\n",
    "\n",
    "allrfpred = allrfpred.replace(bin_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84934c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot PR curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from matplotlib import style\n",
    "\n",
    "#style.use('dark_background')\n",
    "\n",
    "gary_precision, gary_recall, gary_thresholds = precision_recall_curve(allrfpred['quality_class'], allrfpred['predictions'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7feadb8-ec91-4e84-b74a-ba8c9fc2bb62",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'winequality-red-undelimited-preprocessed_2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munder_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomUnderSampler\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# In[4]:\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# In[5]:\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m red_wine \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwinequality-red-undelimited-preprocessed_2.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                           \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfixed acidity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvolatile acidity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcitric acid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresidual sugar\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchlorides\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfree sulfur dioxide\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal sulfur dioxide\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdensity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpH\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msulphates\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43malcohol\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabove_average\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#red_wine['above_average'] = red_wine['above_average'].apply(str)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# In[6]:\u001b[39;00m\n\u001b[1;32m     34\u001b[0m red_wine\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'winequality-red-undelimited-preprocessed_2.csv'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[196]:\n",
    "\n",
    "\n",
    "#import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "#os.chdir('C:\\\\Users\\\\An-94\\\\desktop\\\\ucr\\\\CS235\\\\project')\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "red_wine = pd.read_csv(\"winequality-red-undelimited-preprocessed_2.csv\", \n",
    "                           usecols = ['fixed acidity','volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'above_average'])\n",
    "\n",
    "#red_wine['above_average'] = red_wine['above_average'].apply(str)\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "red_wine.head(5)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.20)\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "x_train, y_train = rus.fit_resample(x_train, y_train)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "def partition(data, feature, boundary): #create a function the partition's data based on where it falls on the boundary condition\n",
    "    left = data[data[feature] >= boundary ]\n",
    "    right = data[data[feature] < boundary ]\n",
    "    \n",
    "    return left, right\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "def gini(data, target): #the gini impurity for calculating the impurity of parent, and child nodes\n",
    "\n",
    "    counts = data[target].value_counts()\n",
    "    impurity = 1\n",
    "    \n",
    "    for c in counts:\n",
    "        prob_of_label = c/float(len(data))\n",
    "        impurity -= prob_of_label**2\n",
    "        \n",
    "    return impurity\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "def info_gain(left, right, current_uncertainty, target): #the function to calculate information gain\n",
    "\n",
    "    p = float(len(left))/(len(left)+len(right))\n",
    "    split_uncertainty = p*gini(left, target) + (1-p)*gini(right, target)\n",
    "    ig = current_uncertainty - split_uncertainty\n",
    "    \n",
    "    return ig\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "def best_split(data, target): #pick the best feature and boundary combination that produces the highest information gain \n",
    "    #initialize the best gain, feature, and boundary variables\n",
    "    best_gain = 0\n",
    "    best_feature = None\n",
    "    best_boundary = None\n",
    "    current_uncertainty = gini(data, target)\n",
    "    features = data.columns.drop(target)\n",
    "\n",
    "    for f in features: #interate over features\n",
    "        if best_feature == None:\n",
    "            best_feature = f\n",
    "            \n",
    "        boundaries = list(set(data[f]))\n",
    "\n",
    "        for b in boundaries: #interate over boundaries\n",
    "            if best_boundary == None:\n",
    "                best_boundary = b\n",
    "                \n",
    "            tr, fr = partition(data, f, b) #output the true rows and false rows at feature and boundary combination\n",
    "            \n",
    "            if (len(tr) == 0) or (len(fr) == 0): #if there are no true rows or false rows continue\n",
    "                continue\n",
    "\n",
    "            gain = info_gain(tr, fr, current_uncertainty, target) #fnd the information gain at the current impurity level\n",
    "\n",
    "            if gain >= best_gain: \n",
    "                #if the current information gain is greater than the best gain then make that the new best gain(along with\n",
    "                #its feature and boundary)\n",
    "                best_gain, best_feature, best_boundary = gain, f, b\n",
    "\n",
    "    return best_gain, best_feature, best_boundary\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "def make_node(data, target, split_level, min_leaf, max_depth): \n",
    "    #create leaf nodes or decision nodes based on whether there was information gain\n",
    "    \n",
    "    gain, feature, boundary = best_split(data, target)\n",
    "    node = {\n",
    "            'info_gain': gain, \n",
    "            'size': len(data), \n",
    "            'feature': feature, \n",
    "            'boundary': boundary\n",
    "           }\n",
    "    # create a node variable with keys info_gain, size, feature, and boundary\n",
    "    \n",
    "    if (gain == 0) or (len(data) <= min_leaf) or (split_level > max_depth):\n",
    "        node['node_type'] = 'leaf' #if all conditions are met then create a node type called leaf\n",
    "        prediction = {} #create a dictionary called prediction\n",
    "        counts = data[target].value_counts() #get he counts of the classes\n",
    "        prediction = 0 \n",
    "        prediction_chance = 0\n",
    "        for i in counts.index: #for rows in the counts index which consists of two classes\n",
    "            prediction_chance_i = counts[i]/float(len(data)) #find the general probability of the class at that index\n",
    "            if prediction_chance_i > prediction_chance: #if the general prob is greater than the current prob \n",
    "                prediction_chance = prediction_chance_i #assign the new prob to prediction chance\n",
    "                prediction = i #and make row equal to the prediction\n",
    "        node['prediction'] = prediction #after that assign the prediction row class to column prediction\n",
    "        node['prediction_chance'] = prediction_chance #and assign the new probability to the prediction chance column \n",
    "        \n",
    "    elif gain > 0:\n",
    "        node['node_type'] = 'branch' #if the information gain is greater than zero then create a node type called a branch\n",
    "        \n",
    "    return node\n",
    "\n",
    "\n",
    "# In[106]:\n",
    "\n",
    "\n",
    "def dtree(data = {}, decision_tree = {}, \n",
    "          split_level = 0, max_depth = 8, min_leaf = 5, \n",
    "          target = 'above_average'):\n",
    "    \n",
    "    if split_level == 0: #if the depth is at the root node than continue\n",
    "        root = make_node(data, target, split_level, min_leaf, max_depth) #create a node and set it to the root variable\n",
    "        root['node_type'] = 'root' #create a column in root variable called 'node type' and assign string 'root' to it\n",
    "        decision_tree[split_level] = [(data, root)] \n",
    "        #the decision tree is a dictionary whever every level consists of data and node; the data is the partitioned data for\n",
    "        #each node\n",
    "        \n",
    "    decision_tree[split_level + 1] = [] #add 1 to the split level after assignment of data and node to the root\n",
    "    keep_splitting = 0 \n",
    "    \n",
    "    \n",
    "    for d, n in decision_tree[split_level]:#d is the interator for the data and n is the interator for the nodes\n",
    "        \n",
    "        if (n['node_type'] != 'leaf'): #if the node is not a leaf it is a decision node\n",
    "            \n",
    "            gain, feature, boundary = best_split(d, target) #assign the best gain, feature and boundary at best split\n",
    "            tr, fr = partition(d, feature, boundary) #assign true rows and false rows at that split\n",
    "\n",
    "            right_branch = make_node(tr, target, split_level + 1, min_leaf, max_depth) #create the right branch of the tree with\n",
    "            #true rows and add one to split level\n",
    "            n['right_child_feature'] = right_branch['feature'] #for the node create a column right child feature and assign\n",
    "            # the feature variable for the right branches\n",
    "            n['right_child_boundary'] = right_branch['boundary'] #for the node create a column right child boundary and assign\n",
    "            # the boundary condition for the right branches\n",
    "            decision_tree[split_level + 1].append((tr, right_branch)) #append to the decision tree the true rows and the right\n",
    "            #branch and add one to the split level\n",
    "            if (right_branch['node_type'] == 'branch'): #if the node type is a right branch keep splitting\n",
    "                keep_splitting = 1 \n",
    "\n",
    "            left_branch = make_node(fr, target, split_level + 1, min_leaf, max_depth) \n",
    "            n['left_child_feature'] = left_branch['feature']\n",
    "            n['left_child_boundary'] = left_branch['boundary']\n",
    "            decision_tree[split_level + 1].append((fr, left_branch)) #do the same things but for the left branch which is for \n",
    "            #the false rows\n",
    "            if (left_branch['node_type'] == 'branch'): #if the node is a left branch keep splitting\n",
    "                keep_splitting = 1\n",
    "    \n",
    "    split_level += 1 #after the left branches and right branches are created move to the next depth level\n",
    "    #print('tree level', split_level, 'complete')\n",
    "    \n",
    "    if (keep_splitting == 1): #a recursive function that output the results of the decision tree at each split level\n",
    "        decision_tree, split_level = dtree(decision_tree = decision_tree, split_level = split_level)\n",
    "        \n",
    "    return decision_tree, split_level\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "node = dtree(red_wine)\n",
    "\n",
    "\n",
    "# In[46]:\n",
    "\n",
    "\n",
    "#node[0][0] root node\n",
    "#node[0][1] 2 branches 2 nodes\n",
    "node[0][2] #4 branches 4 nodes\n",
    "\n",
    "\n",
    "# In[100]:\n",
    "\n",
    "\n",
    "def dtree_predict(data, tree, target = 'above_average'):\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    for i in range(len(data)): #iterate over the length of the data\n",
    "        \n",
    "        x = data.iloc[i] #assign the data at each i row to x\n",
    "        \n",
    "        for s in range(len(tree)): #interate over the split levels of the tree dictionary\n",
    "            \n",
    "            if (s == 0): #if the level is at the root node\n",
    "                node = tree[s][0][1]\n",
    "                # the first index represent the whole tree, the second represent the # of branches of the tree, and the last \n",
    "                #represents a single node at the split level in a branch\n",
    "                current_feature = node['feature'] #set the current feature and boundary\n",
    "                current_boundary = node['boundary']\n",
    "                \n",
    "            for n in range(len(tree[s])): #for node in each length split levels\n",
    "                \n",
    "                node = tree[s][n][1]\n",
    "                \n",
    "                if ((node['feature'] == current_feature) and (node['boundary'] == current_boundary)):\n",
    "                    \n",
    "                    if (node['node_type']!='leaf'):  #if the node is not a leaf \n",
    "                        \n",
    "                        if (x[current_feature] >= current_boundary): \n",
    "                        #if the row in the current feature is greater than the boundary, append the features and boundaries\n",
    "                        #from the right child feature and boundary columns\n",
    "                            next_feature = node['right_child_feature']\n",
    "                            next_boundary = node['right_child_boundary']\n",
    "\n",
    "                        elif (x[current_feature] < current_boundary): #if the row is less append the features and boundaries\n",
    "                        #from the left child\n",
    "                            next_feature = node['left_child_feature']\n",
    "                            next_boundary = node['left_child_boundary'] \n",
    "                            \n",
    "                    if (node['node_type']=='leaf'):\n",
    "                        prediction = node['prediction'] #if the node type is a leaf make a prediction\n",
    "        \n",
    "            current_feature = next_feature #when the loop is done advance to the next level and \n",
    "            current_boundary = next_boundary\n",
    "        \n",
    "    predictions[i] = prediction\n",
    "    data['predictions'] = pd.Series(predictions) #assign the prediction at the instance to the predictions column in the dataframe\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# In[48]:\n",
    "\n",
    "\n",
    "def get_metrics(pred, target='above_average', positive_class= 1):\n",
    "\n",
    "    correct_preds = pred[pred['above_average']==pred['predictions']] #true positive or true negative\n",
    "    correct = len(correct_preds) # # of correct predictions\n",
    "    correct_pos_preds = correct_preds[correct_preds['above_average']== 1] #true positives\n",
    "    correct_positive = len(correct_pos_preds) # # of true positives\n",
    "    positive_actual = len(pred[pred['above_average']== 1]) #counts where real value is positive\n",
    "    positive_pred = len(pred[pred['predictions']== 1]) #counts where predicted value is positive\n",
    "\n",
    "    total = len(pred) #total number of predictions\n",
    "    acc = correct/total #percent of predictions that were correct\n",
    "\n",
    "    precision = correct_positive / positive_pred #percentage of how often a classifier is right about the prediction\n",
    "    recall = correct_positive/positive_actual #percentage of how often a classifier successfully predict the real value\n",
    "    f1 = 2*precision*recall/(precision+recall) #harmonic mean of precision and recall\n",
    "    print('accuracy:',round(acc, 4),', precision:',round(precision, 4),', recall:',round(recall, 4),', f1:',round(f1, 4))\n",
    "    \n",
    "    return acc, precision, recall, f1\n",
    "\n",
    "\n",
    "# In[194]:\n",
    "\n",
    "\n",
    "def confusionator(pred, target='above_average'):\n",
    "    \n",
    "    cm = {}\n",
    "    classes = list(set(pred[target]))\n",
    "    correct_preds = pred[pred[target]==pred['predictions']]\n",
    "    incorrect_preds = pred[pred[target]!=pred['predictions']]\n",
    "    for c in classes:\n",
    "        cm['# correct for actual class: '+str(c)] = len(correct_preds[correct_preds[target]==c])\n",
    "        cm['# incorrect for actual class: '+str(c)] = len(incorrect_preds[incorrect_preds[target]==c])\n",
    "        \n",
    "    return cm\n",
    "\n",
    "\n",
    "# In[101]:\n",
    "\n",
    "\n",
    "x = red_wine.loc[:,red_wine.columns != \"above_average\"]\n",
    "y = red_wine[\"above_average\"]\n",
    "\n",
    "\n",
    "# In[102]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold # import KFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10) \n",
    "skf.get_n_splits(x.copy(), y.copy()) # returns the number of splitting iterations in the cross-validator\n",
    "print(skf)\n",
    "\n",
    "\n",
    "# In[229]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold  # import KFold\n",
    "\n",
    "fold = 1\n",
    "skf_df = {} \n",
    "\n",
    "skf = StratifiedKFold(n_splits=10) \n",
    "skf.get_n_splits(x.copy(), y.copy())\n",
    "\n",
    "for train_index, test_index in skf.split(x, y):\n",
    "\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "\n",
    "    x_train, y_train = rus.fit_resample(x.iloc[train_index], y.iloc[train_index])\n",
    "\n",
    "    train_df = pd.concat([x_train, y_train], axis=1).reset_index(drop=True)\n",
    "    \n",
    "    test_df = pd.concat([x.iloc[test_index], y.iloc[test_index]], axis=1).reset_index(drop=True)\n",
    "    skf_df[fold] = [train_df, test_df]\n",
    "    fold += 1\n",
    "#skf_df[fold #][0=train, 1=test]\n",
    "\n",
    "\n",
    "# In[237]:\n",
    "\n",
    "\n",
    "dt_cv_results_minority = [] #initilize the lists the will be result of the cross validation\n",
    "#dt_cv_results_weighted = []\n",
    "tpreds_list = []\n",
    "\n",
    "for fold in skf_df: #iterate over folds in the dictionary skf_df in which the indexes of the x and y cols and train and testsets are stored\n",
    "    kf_train_df = skf_df[fold][0] #iterated over the selected train dataframes and assign to a variable\n",
    "    kf_test_df = skf_df[fold][1] #iterated over the selected test dataframes and assign to a variable\n",
    "    \n",
    "    dt, sl = dtree(data=kf_train_df, target='above_average') #put the training dataset into the tree function\n",
    "    tpred = dtree_predict(kf_test_df, dt, target = 'above_average') #take the prediction results and compare with test_data\n",
    "    tpreds_list.append(tpred)\n",
    "    \n",
    "    fold_results_minority = [get_metrics(pred=tpred, target='above_average', positive_class= 1)]\n",
    "    dt_cv_results_minority.append(fold_results_minority)\n",
    "    \n",
    "    #fold_results_weighted = [get_weighted_metrics(pred= tpred, target='above_average')]\n",
    "    #dt_cv_results_weighted.append(fold_results_weighted)\n",
    "\n",
    "\n",
    "# In[238]:\n",
    "\n",
    "\n",
    "tpreds_list\n",
    "\n",
    "\n",
    "# In[239]:\n",
    "\n",
    "\n",
    "tpreds_folded = pd.concat(tpreds_list, ignore_index = True)\n",
    "tpreds_folded\n",
    "\n",
    "\n",
    "# In[225]:\n",
    "\n",
    "\n",
    "a = dt_cv_results_minority\n",
    "b = np.asarray(a)\n",
    "c = np.mean(b, axis = 1)\n",
    "\n",
    "\n",
    "d = pd.DataFrame(c)\n",
    "d\n",
    "\n",
    "per_report_df = d.rename(columns={ 0 : \"accuracy\",  1 : \"precision\",  2 : \"recall\", 3 : \"f1\"})\n",
    "per_report_mean  = e.mean(axis = 0)\n",
    "print(e)\n",
    "print(f)\n",
    "\n",
    "\n",
    "# In[226]:\n",
    "\n",
    "\n",
    "per_report_df\n",
    "\n",
    "\n",
    "# In[227]:\n",
    "\n",
    "\n",
    "per_report_mean \n",
    "\n",
    "\n",
    "# In[228]:\n",
    "\n",
    "\n",
    "per_report_stdev = e.std(axis = 0)\n",
    "per_report_stdev\n",
    "\n",
    "\n",
    "# In[195]:\n",
    "\n",
    "\n",
    "cm = confusionator(tpred)\n",
    "cm\n",
    "\n",
    "\n",
    "# In[222]:\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "confusion_matrix(tpred['above_average'], tpred['predictions'])\n",
    "\n",
    "\n",
    "# In[224]:\n",
    "\n",
    "\n",
    "\n",
    "print(classification_report(tpred['above_average'], tpred['predictions']))\n",
    "\n",
    "\n",
    "# In[240]:\n",
    "\n",
    "\n",
    "tpreds_folded = pd.concat(tpreds_list, ignore_index = True)\n",
    "tpreds_folded\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#plot a precision recall curve \n",
    "\n",
    "\n",
    "# In[246]:\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from matplotlib import style\n",
    "\n",
    "style.use('default')\n",
    "pd.set_option('display.max_columns', 150)\n",
    "pd.set_option('display.max_rows', 150)\n",
    "style.use('default')\n",
    "\n",
    "tn_precision, tn_recall, tn_thresholds = precision_recall_curve(tpreds_folded['above_average'], tpreds_folded['predictions'])\n",
    "#plt.figure(figsize=(12, 7))\n",
    "#plt.plot(tn_recall, tn_precision)\n",
    "#plt.xlabel('Recall (Positive Label: above_avg)', fontsize=8)\n",
    "#plt.ylabel('Precision (Positive Label: above_avg)', fontsize=8)\n",
    "#ticks = [.25, .5, .75, 1.00]\n",
    "#plt.xticks(ticks, ticks, fontsize=8)\n",
    "#plt.yticks(ticks, ticks, fontsize=8)\n",
    "#plt.savefig('precision_recall_curve_tn.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ec95f64-6673-43fa-b804-4ab77b5e4b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class naivebayes:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.var_mean = {}\n",
    "        self.prob_c = {}\n",
    "        self.attribute_probabilities = []\n",
    "        self.t = None\n",
    "    \n",
    "    #X and y inputs must be pandas dataframe or numpy arrays    \n",
    "    def fit(self,X,y, mle = True, priors = [], t = None):\n",
    "        import numpy as np\n",
    "        labels = np.unique(y)\n",
    "        self.t = t\n",
    "        #estimate class priors with the option to use predefined priors\n",
    "        if priors == []:\n",
    "            probabilities_of_c = [float(sum(c == y))/len(y) for c in labels]\n",
    "        else:\n",
    "            probabilities_of_c = priors\n",
    "        #append class probabilities into predefined dictionary\n",
    "        for n,label in enumerate(labels):\n",
    "            self.prob_c[label] = probabilities_of_c[n]\n",
    "        \n",
    "                   \n",
    "        #variance and mean for each attribute in a class\n",
    "        for label in labels:\n",
    "            self.var_mean[label] = []\n",
    "            matching = X[np.where(y == label)]\n",
    "            for i in range(len(matching[0])):\n",
    "                # m is the \"coefficient\" for the maximum likelihood estimation of variance\n",
    "                if mle == True:\n",
    "                    m = (len(matching.take(i,1)) - 1 )/ len(matching.take(i,1))\n",
    "                else:\n",
    "                    m = 1\n",
    "                var = np.var(matching.take(i,1), ddof = 1) * m\n",
    "                mean = np.mean(matching.take(i,1))\n",
    "                self.var_mean[label].append([var,mean])       \n",
    "                \n",
    "    #Predicts labels given a set of vectors, either numpy arrays or pandas series\n",
    "    def gauss_predict(self, vectors):\n",
    "        import numpy as np\n",
    "        self.attribute_probabilities = []\n",
    "        \n",
    "        #calculate datapoints' probability given a label of class c\n",
    "        labels = []\n",
    "        for i in self.prob_c.keys():\n",
    "            attribute_probability = []\n",
    "            for n in range(len(vectors[0])):\n",
    "                var = self.var_mean[i][n][0]\n",
    "                mean = self.var_mean[i][n][1]\n",
    "                #I split up the calculation below so that it was tidier/easier to read\n",
    "                coefficient = 1.0 / ((2.0*np.pi*var)**0.5)\n",
    "                gauss = lambda x:coefficient * (np.e**(-((x - mean)**2.0)/(2.0*var)))\n",
    "                #applies the gaussian probability to each datapoint in an attribute\n",
    "                ap = [gauss(d) for d in vectors.take(n,1)]\n",
    "                attribute_probability.append(ap)\n",
    "            #finds the product of all conditionally independent probabilities\n",
    "            attribute_probability = np.prod(attribute_probability,axis = 0)\n",
    "            #multiplies each probability by the class prior probability\n",
    "            attribute_probability = attribute_probability * self.prob_c[i]\n",
    "            self.attribute_probabilities.append(attribute_probability)\n",
    "        attribute_probabilities = np.array(self.attribute_probabilities)\n",
    "        #Prediction \n",
    "        classes = [i for i in self.prob_c.keys()]\n",
    "        if self.t is None:\n",
    "            for i in range(len(attribute_probabilities[0])):\n",
    "                if attribute_probabilities[0][i] > attribute_probabilities[1][i]:\n",
    "                    labels.append(classes[0])\n",
    "                if attribute_probabilities[0][i] < attribute_probabilities[1][i]:\n",
    "                    labels.append(classes[1])\n",
    "            return labels \n",
    "        else:\n",
    "            for i in range(len(attribute_probabilities[0])):\n",
    "            #this was added for ease of thresholding\n",
    "                total_prob = sum(attribute_probabilities.take(i,1))\n",
    "                normalized_ap = attribute_probabilities.take(i,1)/total_prob\n",
    "            #determine label\n",
    "                biggest_prob = max(normalized_ap)\n",
    "                label = int(np.where(attribute_probabilities == max(attribute_probabilities.take(i,1)))[0][0])\n",
    "            #thresholding\n",
    "                if biggest_prob < self.t and label == 0: \n",
    "                    labels.append(1)    \n",
    "                else:\n",
    "                    labels.append(classes[label])\n",
    "            return labels \n",
    "    \n",
    "    #shows the probabilities for each class for each datapoint\n",
    "    def predict_proba(self, vectors):\n",
    "        import numpy as np\n",
    "        self.gauss_predict(vectors)\n",
    "        attribute_probabilities = np.array(self.attribute_probabilities)\n",
    "        normalized_ap = []\n",
    "        for i in range(len(attribute_probabilities[0])):\n",
    "            total_prob = sum(attribute_probabilities.take(i,1))\n",
    "            normalized_ap.append(attribute_probabilities.take(i,1)/total_prob)\n",
    "        probs = np.array([[i[0],i[1]] for i in normalized_ap])\n",
    "        return probs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eaf403b2-3359-4c1c-8772-090b6190596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('winequality-red.csv', delimiter = ';')\n",
    "q = lambda x:1 if x>= 7 else 0\n",
    "df['binary_quality'] = df['quality'].apply(q)\n",
    "data = df[[ 'volatile acidity', 'citric acid', 'residual sugar',\n",
    "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
    "       'sulphates']]\n",
    "labels = df['binary_quality']\n",
    "#choice_data = df[['volatile acidity', 'citric acid', 'residual sugar',\n",
    " #                 'free sulfur dioxide','total sulfur dioxide', 'pH', 'sulphates']]\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "#scaler = StandardScaler()\n",
    "scaled_data = scale(data, axis = 0)\n",
    "#scaled_data = scaler.fit_transform(choice_data)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = naivebayes()\n",
    "sk = GaussianNB()\n",
    "features = ['volatile acidity', 'citric acid', 'residual sugar',\n",
    "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
    "        'sulphates', 'alcohol']\n",
    "d = df[features]\n",
    "X = scale(d, axis = 0)\n",
    "y = df['binary_quality']\n",
    "rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=10,\n",
    "    random_state=119)\n",
    "myp = []\n",
    "myr = []\n",
    "mya = []\n",
    "skp = []\n",
    "skr = []\n",
    "ska = []\n",
    "avgp = []\n",
    "avgr = []\n",
    "avga = []\n",
    "skavgp = []\n",
    "skavgr = []\n",
    "skavga = []\n",
    "prob_error = []\n",
    "for train_index, test_index in rskf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index] \n",
    "    nb.fit(X_train,y_train)\n",
    "    sk.fit(X_train,y_train)\n",
    "    y_pred = nb.gauss_predict(X_test)\n",
    "    sky_pred = sk.predict(X_test)\n",
    "    avg_error = sum(nb.predict_proba(X_test).take(1,1) - sk.predict_proba(X_test).take(1,1)) / len(nb.predict_proba(X_test).take(1,1))\n",
    "    prob_error.append(avg_error)\n",
    "    myp.append(precision_score(y_test, y_pred))\n",
    "    myr.append(recall_score(y_test, y_pred))\n",
    "    mya.append(accuracy_score(y_test, y_pred))\n",
    "    skp.append(precision_score(y_test, sky_pred))\n",
    "    skr.append(recall_score(y_test, sky_pred))\n",
    "    ska.append(accuracy_score(y_test, sky_pred))\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d27df2c-c14d-4183-a534-96beb77af8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtYAAAGsCAYAAAD5QeD8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABhaklEQVR4nO3dd3RU1frG8e9OD6SQhJ5C7y1UCb2JIEVpYhdUir170Xuv4u+q14IN9YqoiAUbFsSChV4EERCwgCK9Q2ghQCBl//44kxAwkAQmOSnPZ61ZnDnnzMybyazwZOc9extrLSIiIiIicn583C5ARERERKQkULAWEREREfECBWsRERERES9QsBYRERER8QIFaxERERERL/BzuwBvKV++vK1evbrbZYiIiIhICbd8+fJEa22F0/eXmGBdvXp1li1b5nYZIiIiIlLCGWM257RfrSAiIiIiIl6gYC0iIiIi4gUK1iIiIiIiXlBieqxFREREiqrU1FS2bdtGSkqK26VIPgQFBRETE4O/v3+ezlewFhERESlg27ZtIzQ0lOrVq2OMcbscyQNrLfv27WPbtm3UqFEjT49RK4iIiIhIAUtJSSEqKkqhuhgxxhAVFZWvvzIoWIuIiIgUAoXq4ie/3zMFaxERERERL1CwFhERESkFjDHcc889WffHjRvH2LFjz/qY6dOn88QTT5z3a0+ePJkKFSoQHx9Po0aNGDx4MEePHj3v5y1qFKxFRERESoHAwEA+/fRTEhMT8/yY/v37M2bMGK+8/tChQ1m5ciW//fYbAQEBfPjhh1553qJEwVpERESkFPDz82PkyJE899xzfzv2xRdfcMEFF9C8eXN69OjB7t27AWek+dZbb+XQoUNUr16djIwMAI4ePUpsbCypqamsX7+eXr160bJlSzp27MjatWvPWkdaWhpHjhwhIiLijK+dkZFBnTp12Lt3LwAZGRnUrl2bxMRE9u7dy6BBg2jdujWtW7dm0aJFAMybN4/4+Hji4+Np3rw5hw8f9tp7l1eabk9ERESkED3yxW/8viPJq8/ZsGoYD/drlOt5t9xyC02bNuX+++8/ZX+HDh1YsmQJxhhef/11nnrqKZ555pms4+Hh4TRr1ox58+bRtWtXvvjiCy666CL8/f0ZOXIkEyZMoE6dOvz444/cfPPNzJ49+2+v/eGHH7Jw4UJ27txJ3bp16dev31lf++qrr2bKlCnceeedzJw5k2bNmlG+fHmuvPJK7rrrLjp06MCWLVu46KKLWLNmDePGjePll1+mffv2JCcnExQUdJ7vav4pWIuIiIiUEmFhYVx77bWMHz+e4ODgrP3btm1j6NCh7Ny5kxMnTuQ4b/PQoUP58MMP6dq1Kx988AE333wzycnJ/PDDDwwZMiTrvOPHj+f42kOHDuWll17CWsstt9zC008/zZgxY8742tdffz2XXHIJd955J5MmTWL48OEAzJw5k99//z3reZOSkjh8+DDt27fn7rvv5qqrrmLgwIHExMR45T3LD1eDtTGmKvAl0BAIsdamnXbsXSAIeMhaO9OdKkVERES8Jy8jywXpzjvvpEWLFllBFeC2227j7rvvpn///sydOzfHixr79+/PAw88wP79+1m+fDndunXjyJEjlCtXjpUrV+b59Y0x9OvXjxdffJExY8ac8bVjY2OpVKkSs2fP5scff2TKlCmA0xayePHiU34xABgzZgx9+vTh66+/pm3btsycOZP69evn+/05H26PWO8HugOf5XBsDPAvYDVO+C5ywfroiTQSf5vHsSpt3C5FirHQID+qlgvO/UQREREviIyM5LLLLuONN97g+uuvB+DQoUNER0cD8NZbb+X4uJCQENq0acMdd9xB37598fX1JSwsjBo1ajB16lSGDBmCtZbVq1fTrFmzs9awcOFCatWqletr33jjjVx99dVcc801+Pr6AtCzZ09eeukl7rvvPgBWrlxJfHw869evp0mTJjRp0oTFixezdu3a0hWsrbUpQMoZJt9uCtxhrbXGmMPGmFBrbeF3oZ/F1h+nUW/WDUxN68RDacM4RuH38kjJMP++rsRFlXG7DBERKSXuueceXnrppaz7Y8eOZciQIURHR9O2bVs2btyY4+OGDh3KkCFDmDt3bta+KVOmcNNNN/Hoo4+SmprK5ZdfnmOwzuyxzsjIICYmhsmTJ+f62v3792f48OGnjK6PHz8+q1c8LS2NTp06MWHCBJ5//nnmzJmDr68vDRs2pHfv3uf5LuWfsdYW+ov+rQhj5gI9TmsFWWCt7ejZfhd40Fq75bTHjQRGAsTFxbXcvHlz4RUN7Es6yv4Z/6H2mldIDq3Fz22fIzmsTqHWIMXbqm0HeXXeBr68rQONo8PdLkdERArImjVraNCggdtlFDvLli3jrrvuYsGCBa7VkNP3zhiz3Frb6vRz3W4FOZv0bNthwMHTT7DWTgQmArRq1arQf0OICitD1ND/woaLCP1kBJ3mXAZ9xkH8VaBlSyUP/H19eJUNbpchIiJS5DzxxBO88sorWb3VxUFRnsd6tTEmwRhTFgiz1np3XhpvqtkFRi+E2Nbw+S3w2Wg4nux2VSIiIiLF1pgxY9i8eTMdOnRwu5Q8czVYG2P8jTEzgWbAt8aYC4wxL3oOPwU8hnPR4uNu1ZhnoZXgmmnQ5QFY/SG81hV2/57rw0RERESkZHD74sVUoMdpu3/0HNsGdCv0os6Hjy90GQPV2sEnNzrh+uKnofk1ag0RERERKeGKcitI8VWjk9MaEtcWpt8Gn45Ua4iIiIhICadgXVBCKsLVn0LXf8KvH8PELrDrV7erEhEREZEComBdkHx8ofP9cN0XcPwwvN4dlr0JRWCKQxERESldfH19iY+Pp3HjxvTr14+DBw965XknT57Mrbfe6pXnyq5Lly7Uq1eP+Ph44uPj+fjjj73+GgCbNm3ivffe88pzKVgXhuodnNaQau3gyzud/uvjRWqtGxERESnhgoODWblyJb/++iuRkZG8/PLLbpeUqylTprBy5UpWrlzJ4MGD8/SYtLS03E/KRsG6OAqpAFd9At3+Db99Cq92hp2r3a5KRERESqGEhAS2b98OwNKlS2nXrh3NmzenXbt2/PHHH4AzEj1w4EB69epFnTp1uP/++7Me/+abb1K3bl06d+7MokWLsvZv3ryZ7t2707RpU7p3786WLc7afsOGDeOmm26ia9eu1KxZk3nz5nH99dfToEEDhg0blue69+/fz6WXXkrTpk1p27Ytq1c7WWrs2LGMHDmSnj17cu2117J3714GDRpE69atad26dVaN8+bNyxoBb968OYcPH2bMmDEsWLCA+Ph4nnvuufN6X4vyAjElj48PdLoX4hLgkxvg9R7Q67/Q6nrNGiIiIlJazBgDu37x7nNWbgK9n8jTqenp6cyaNYsbbrgBgPr16zN//nz8/PyYOXMmDz74IJ988gkAK1eu5OeffyYwMJB69epx22234efnx8MPP8zy5csJDw+na9euNG/eHIBbb72Va6+9luuuu45JkyZx++23M23aNAAOHDjA7NmzmT59Ov369WPRokW8/vrrtG7dmpUrVxIfH/+3Wq+66iqCg4MBmDVrFmPHjqV58+ZMmzaN2bNnc+2117Jy5UoAli9fzsKFCwkODubKK6/krrvuokOHDmzZsoWLLrqINWvWMG7cOF5++WXat29PcnIyQUFBPPHEE4wbN44vv/zyPL4BDgVrN1Rv77SGfDYKvrobNi2Efi9AUJjblYmIiEgJdezYMeLj49m0aRMtW7bkwgsvBODQoUNcd911rFu3DmMMqampWY/p3r074eHhADRs2JDNmzeTmJhIly5dqFChAgBDhw7lzz//BGDx4sV8+umnAFxzzTWnjHL369cPYwxNmjShUqVKNGnSBIBGjRqxadOmHIP1lClTaNXq5MrhCxcuzAr93bp1Y9++fRw6dAiA/v37Z4XwmTNn8vvvJ9cTSUpK4vDhw7Rv3567776bq666ioEDBxITE3Me7+jfKVi7pWx5uHIq/PACzPoP7FwJQyZDlWZuVyYiIiIFKY8jy96W2WN96NAh+vbty8svv8ztt9/Ov//9b7p27cpnn33Gpk2b6NKlS9ZjAgMDs7Z9fX2z+pdNHv/Snv28zOfy8fE55Xl9fHzy3Bdtc5gAIvM1ypYtm7UvIyODxYsXZwXtTGPGjKFPnz58/fXXtG3blpkzZ+bpdfNKPdZu8vGBDnfBsK8g7bjTGrL0Nc0aIiIiIgUmPDyc8ePHM27cOFJTUzl06BDR0dGA01edmwsuuIC5c+eyb98+UlNTmTp1ataxdu3a8cEHHwDOaLO3lyPv1KkTU6ZMAWDu3LmUL1+esLC//8W/Z8+evPTSS1n3M9tF1q9fT5MmTfjHP/5Bq1atWLt2LaGhoRw+7J1JJRSsi4JqCTBqAdTsAl/fC1Ovg5RDblclIiIiJVTz5s1p1qwZH3zwAffffz8PPPAA7du3Jz09PdfHVqlShbFjx5KQkECPHj1o0aJF1rHx48fz5ptv0rRpU9555x1eeOEFr9Y9duxYli1bRtOmTRkzZgxvvfVWjueNHz8+67yGDRsyYcIEAJ5//nkaN25Ms2bNCA4Opnfv3jRt2hQ/Pz+aNWt23hcvmpyG1IujVq1a2WXLlrldxvnJyIDFL8LMR6BcrNMaUrW521VJAfr+992MeHsZX97WgcbR4W6XIyIiBWTNmjU0aNDA7TLkHOT0vTPGLLfWtjr9XI1YFyU+PtD+Dhg+A9LT4I2e8ONEtYaIiIiIFAMK1kVR3AUwegHU6gYz7oOProFjB92uSkRERETOQsG6qCoTCVd8AD0fhT9mwKudYPtyt6sSERGRc1RS2m9Lk/x+zxSsizJjoN1tMPwbsBnwxkWw5BW1hoiIiBQzQUFB7Nu3T+G6GLHWsm/fPoKCgvL8GM1jXRzEtoZR8+HzW+CbMc6CMpe8BMERblcmIiIieRATE8O2bdvYu3ev26VIPgQFBeVrERkF6+KiTCRc/h4s+R98/zBM6OTMGhLT0u3KREREJBf+/v7UqFHD7TKkgKkVpDgxBhJugeu/BQNM6gk/vKTWEBEREZEiQMG6OIpp6bSG1O0F3/0TPrgSju53uyoRERGRUk3BurgKjoCh70KvJ2Hd986sIVt/crsqERERkVJLwbo4MwbajoYbvgXjA2/2gkXjnRUcRURERKRQKViXBNGe1pB6F8P3/4YPrlBriIiIiEghU7AuKYLLwWVvw8XjYP1smNARtvzodlUiIiIipYaCdUliDLQZATd8B75+8GZvWPi8WkNERERECoGCdUlUtbnTGtKgH8x8GN67DI7sc7sqERERkRJNwbqkCgp3FpDp8wxsnAcTOsDmxW5XJSIiIlJiKViXZMZA6xvhxpngHwST+8CCZ9UaIiIiIlIAFKxLgyrNYOQ8aHgJzHoE3hsCRxLdrkpERESkRFGwLi2CwmDwJOj7HGxc4LSGbFrkdlUiIiIiJYaCdWliDLS63tMaUgbe6gvzn1ZriIiIiIgXKFiXRlWawqh50GggzH4U3h0IyXvdrkpERESkWFOwLq0CQ2HQ69DvBdiy2GkN2bjA7apEREREii0F69LMGGg5DG6c5QTtt/vD3CchI93tykRERESKHQVrgcqNYeRcaDIE5j4O7wyA5D1uVyUiIiJSrChYiyMwBAa8Cv1fgq0/Oq0hG+a5XZWIiIhIsaFgLScZAy2ugRFznJUb374E5vxXrSEiIiIieaBgLX9XqaETrptdDvOecAL24d1uVyUiIiJSpClYS84CQ2DABLjkf7BtGUxoD+vnuF2ViIiISJGlYC1n1/wqGDkHykQ5FzXOfkytISIiIiI5ULCW3FVsACNmQ/xVMP8peKs/JO10uyoRERGRIkXBWvImoCxc+jJcOgF2rHBmDflrlttViYiIiBQZCtaSP/FXOHNel60A7w6CWf+B9DS3qxIRERFxnYK15F+Fek5rSPOrYcE4eKsfJO1wuyoRERERVylYy7kJKAOXvAQDJsLOVU5ryLqZblclIiIi4hoFazk/zYY6rSEhlWHKIJg5Vq0hIiIiUiopWMv5q1AXRsyCFtfBwufgrb5waLvbVYmIiIgUKgVr8Q7/YOg/Hga9Abt+cVpD/vzO7apERERECo2CtXhXk8Ewch6ERcN7Q+D7hyA91e2qRERERAqcgrV4X/nacOP30Op6WPQCTO4DB7e6XZWIiIhIgVKwloLhHwx9n4PBk2D37/BqR/jjG7erEhERESkwCtZSsBoPglHzIDwW3h8K3/5TrSEiIiJSIilYS8GLqgU3fA+tb4TFL8GbveHgFrerEhEREfEqBWspHP5B0OcZGDIZ9qyFCR1h7dduVyUiIiLiNQrWUrgaDYDR8yGiGnxwBXzzIKSdcLsqERERkfOmYC2FL7Km0xrSZhQseRne7AUHNrtdlYiIiMh5UbAWd/gFwsVPwWVvQ+Jfzqwha750uyoRERGRc6ZgLe5qeIkza0hkTfjwKpgxRq0hIiIiUiwpWIv7ImvA9d/CBTfBj6/ApJ6wf6PbVYmIiIjki4K1FA1+gdD7CRj6LuzfAK92ht+nu12ViIiISJ4pWEvR0qAfjFrgLIv+0TXw9X2QdtztqkRERERypWAtRU9ENRj+DbS9BZZOhDd6OqPYIiIiIkWYgrUUTX4B0OtxuPw9OLDRaQ357TO3qxIRERE5IwVrKdrq94HRC6F8XZg6DL66B1JT3K5KRERE5G8UrKXoKxcHw2dAwq3w0+vwxoWwb73bVYmIiIicQsFaige/ALjoMbjiQzi01WkN+fUTt6sSERERyaJgLcVLvV7OrCEVG8DH18OXd6k1RERERIoEBWspfsrFwvCvof0dsGwSvN7DWRZdRERExEUK1lI8+frDhf8HV06FpO0wsTP88rHbVYmIiEgppmAtxVvdns6sIZUawyc3wPTbIfWY21WJiIhIKaRgLcVfeDQM+wo63A0r3oLXusPeP92uSkREREoZBWspGXz9oMfDcNUnkLwLJnaB1R+5XZWIiIiUIgrWUrLU6eG0hlRpBp+OgM9vhRNH3a5KRERESgEFayl5wqrCdV9Ax3vh53fh9e6w9w+3qxIREZESTsFaSiZfP+j+b7j6E0je47SGrHzf7apERESkBPPLy0nGGB+gC9AZqA4EA3uBFcB31tqtBVSfyPmp3d1pDfnkRpg2GjYthIufgoCyblcmIiIiJcxZR6yNMcHGmH8CW4GvgJ5ACHACqAE8DGw0xnxtjGlb0MWKnJOwKnDt59Dpflg5BV7rBnvWul2ViIiIlDC5tYKsA+KBkUCYtTbBWjvIWnu1tfZia20cUAtYAHxojBlRsOWKnCNfP+j2T7jmMzi6z2kN+XmK21WJiIhICZJbsO5lrR1irf3KWpua0wnW2s3W2v8CdYC53i5QxKtqdXVaQ2Jawec3w2ej4cQRt6sSERGREuCswdpa+2ten8hae8Jau+78SxIpYKGVndaQzmNg1QcwsSvs/t3tqkRERKSYO+9ZQYwxZY0xnbxRjEih8fGFrg84AfvYAafvesXbYK3blYmIiEgx5Y3p9moDc7zwPCKFr2ZnpzUktg1Mvw0+GwXHk92uSkRERIohzWMtElrJuaix6z/hl6nOhY278twFJSIiIgLkIVgbY9LPdsOZy1qkePPxhc73w7XT4XiSs1rj8slqDfGSE2kZbN2vpeVFRKRky8sCMceA54Gfz3C8BvCktwoScVWNjk5ryKcj4Ys7nAVl+j4HgaFuV1Ys/bn7MFOXbeXTFdvZf/QEi/7Rjarlgt0uS0REpEDkJVivBPZZaz/J6aAxphkK1lKShFSEqz+Fhc/AnMdhx88wZDJUbuJ2ZcVCUkoqX67ayUfLtrJy60H8fQ01y4ew78gJDqekuV2eiIhIgclLsP4aCD/L8f3A294pR6SI8PGBTvdBXDv45AZ4rTv0fgJaDgdj3K6uyLHW8uPG/Xz001a+/nUnKakZ1K0Uwr/6NGBA82h+3Lifm6eoa0xEREq2XIO1tfbxXI5vBYZ7rSKRoqR6+5OtIV/eBRsXQL8XICjM7cqKhJ2HjvHJ8m1MXb6NzfuOEhrox6AWMVzWKpamMeEY/RIiIiKlSF5GrEVKt7Ll4aqPYdFzMPsx2LnSaQ2p0sztylxxPC2dmb/v4aNlW1mwbi8ZFhJqRnFnjzr0alSF4ABft0sUERFxhYK1SF74+EDHe5zWkI+vh9d7QK//QqsbSk1ryJqdSXy0bCvTft7OgaOpVAkP4tautRncMpa4qDJulyciIuI6BWuR/KiW4LSGfDYKvrrHaQ3pPx6CznYZQvF16Ggq01dt56Nl2/hl+yECfH24sFElhraKpX3t8vj6lI5fKkRERPJCwVokv8pGwZUfwQ/jYdb/nWwNqdrc7cq8IiPDsnjDPj5atpVvft3F8bQMGlQJY2y/hlwSH01E2QC3SxQRESmSFKxFzoWPD3S4E+LaOq0hb/SEno9BmxHFtjVk24GjfLx8G1OXbWP7wWOEB/tzeetYhrSKpXF0yRyRFxER8aZCDdbGmOeAVsAKa+0d2fY/D8R77jaz1kYYY4YBDwA7gaXW2vsLs1aRPIlr62kNGQ0z7oNNC6D/ixBczu3K8iQlNZ3vft/N1GVbWfhXIgAdapfnH73r07NhJYL8dSGiiIhIXuUrWBtjHgL2WmtfybbvZiDKWvufXB7bAihrre1ojHnFGNPaWvsTgLX2Ts85zYF7sj3saWvt6/mpUaTQlYmEKz6AxS/BrEfg1VVOa0h0C7crO6Nftx/KuhAxKSWNmIhg7uxel0Eto4mJ0IWIIiIi5yK/I9ZjgbXAK9n23Q7UAc4arIEEYKZneybQFvjptHMGAJ9mu3+nMeZa4BFr7ax81ipSeHx8oP3tp7WG/AcuGF2kWkMWr9/H+FnrWLxhHwF+PvRuXJnLWsWSUDMKH12IKCIicl7yFayttT457Kufx4eXA9Z7tg8BjXI4pxfwhGd7Gs6KjlHAd8aYVtba9OwnG2NGAiMB4uLi8liGSAGKbQOj5sO0m+GbMbBpIVzyEgRHuFaStZYf1u/jhVnrWLpxPxVCA/lXnwYMaRlLeBl/1+oSEREpaQqzx/ogkLlcXZjnfhZjTB1gu7X2KIC1NvP4XmPMn0AlYEf2x1hrJwITAVq1amULqG6R/CkTCVe8D0v+B98/BK92gsGTIaZloZZhrWX+ukTGz1rH8s0HqBQWyNh+Dbm8TZx6p0VERArAWYO1MSYyr09krd2fyymLgVHAR0APYPJpxwcAn2V77TBrbZIxJhin1WRvXmsRcZ0xkHALxF4AU4fDpIvgwkeg7c0F3hpirWXuH3t5YdY6Vm49SNXwIP5zaWOGtIxRoBYRESlAuY1YJwK5jQQbzzln/R/bWrvCGJNijFkArLLWLjXGvGitvc1zSl/gkmwPucsY0wvwAZ6w1qbmUodI0RPTCkbPh2m3wLcPelpDXnZGtb3MWsvMNXsYP2sdv2w/RHS5YB4f0ITBLWMI8PtbF5eIiIh4WW7Buqs3Xyz7FHue+7dl2+502rFHgEe8+foirgiOgMunwI8T4Lt/e1pD3oTY1l55+owMy3e/72b8rHX8vjOJuMgyPDWoKQNaROPvq0AtIiJSWM4arK218wqrEJESzRhoe5NzcePUYfBmL+gxFsKHnPNTZmRYZvy6ixdnr2PtrsPUKF+WZ4Y045L4qvgpUIuIiBS6/M5jXQm4BqgF/Ntam2iMaQ/ssNZuLIgCRUqU6JYwagFMvxW++xfxVWZRjsvz9RTpGZavftnJi7PWsW5PMrUqlOX5ofH0bVpFgVpERMRFeQ7WxpiWwCxgI85UeU/j9GBfCNQFriyIAkVKnOBycNk7sHQiUd/+k68CV5OyuwJEdz/rw9LSM/hi9Q5enP0XG/YeoW6lEF68ojkXN6mCr+agFhERcV1+RqzHAS9Yax82xhzOtv9bYLh3yxIp4YyBC0axNLU2Vb4bTc0vhsCxhyHhNmexmWzS0jOYtnIHL8/5i42JR6hfOZT/XdWCXo0qa1EXERGRIiQ/wbolcEMO+3fizDEtIvl0OLIxI048zsIGnxL+/UPOrCGXToCyUaSmZ/Dpim28PGc9W/YfpWGVMCZc3ZKeDSspUIuIiBRB+QnWx4Cclo+rD+zxTjkipc9hyrC1+yuE75gK3z6IndCBmQ3/y9hVYWw/eIymMeE81LcV3RtUxBSh5dFFRETkVPkJ1p8DDxtjMqcxsMaY6sCTwCfeLkykVDGG4y2uZ+aBGJotvpOuS4azM+RaYq97gC71KylQi4iIFAP5Cdb3Al/jrIBYBliI0wKyCPiX90sTKT0+WraV737bza6kDDrEvsSzwZO4dstkWL4N4l6FsuXdLlFERERykedgba1NAjoYY7oBLXBWRFxhrZ1ZUMWJlBZvL95Mm+qRPHNZM9rVisLQA5ZNgm8egAkdYPAkqNbO7TJFRETkLPI1jzWAtXY2MLsAahEpddpUj+TahGpc3KQKbWtGnXqw9Q0Q09pZUGZyH+j6T+hw999mDREREZGiIV//QxtjLjXGzDfGJHpuC4wxAwqqOJGSLryMP/93SeO/h+pMVZrCqHnQaCDM/g9MGQTJewu3SBEREcmTPAdrY8w9wIfAH8D9ntta4D1jzL0FU56IEBgKg16Hfi/ApkVOa8imhW5XJSIiIqfJz4j1vcCt1toR1tpJntsI4HbgnoIpT0QAZ0GZlsNgxCwIDIG3+sG8pyAj3e3KRERExCM/wToEmJPD/jmeYyJS0Co3gZFzofFgmPMYvDsQkjWNvIiISFGQn2A9DRicw/5BwHSvVCMiuQsMhYETof+LsGWJ0xqycb7bVYmIiJR6Z50VxBhzd7a7fwFjjDFdgcWefW09t2cLpjwRyZEx0OJaiG7pzBry9iXQ+R/Q6T7w8XW7OhERkVIpt+n2bjvt/gGgrueWfd8w4P+8V5aI5EmlRjBiDnx1D8z9L2xeBANfh9BKblcmIiJS6pw1WFtraxRWISJyjgJDYMAEqNERvrrXaQ0Z9BrU7OJ2ZSIiIqWKVpoQKQmMgeZXw8g5EBwBb18Kcx7XrCEiIiKFKF8rLxpj6uJcwBgHBGQ/Zq293ot1ici5qNjACddf3wfznoTNPzhzYIdWdrsyERGREi8/C8T0AVYD/YDrgXrAxcAAoHyBVCci+RdQFi79H1z6Cmxf7rSGrJ/tdlUiIiIlXn5aQf4PeMRamwAcB64BqgMzgbler0xEzk/8lc6FjWXKwzsDYdZ/ID3N7apERERKrPwE63o4S5oDpAJlrLUpOIH7Ti/XJSLeULE+jJgNza+CBePg7f6QtMPtqkREREqk/ATrw0CQZ3snUNuz7QdEeLMoEfGigDJwycsw4FXYsdJpDflrpttViYiIlDj5CdY/Ah08218BzxhjHgbe5OSCMSJSVDW73FkOPaQSvDsIZj6i1hAREREvyk+wvhtY4tkeC3yHs5z5X8CN3i1LRApEhbpw4yxn1caFz8JbfeHQdrerEhERKRHyHKyttRustas920ettTdZa5sCVwEhBVWgiHhZQBno/6KzQuOuX5zWkHXfu12ViIhIseeNBWLqA7944XlEpDA1HeK0hoRVhSmD4fuHID3V7apERESKLa28KFKala8DN86ElsNh0QswuQ8c2uZ2VSIiIsWSgrVIaecfDP2eh0FvwO7fnNaQP75xuyoREZFiR8FaRBxNBsOo+RAWA+8Phe/+pdYQERGRfPDL7QRjTItcTqnnpVpExG1RtZzWkG8fhB9ehC1LYPAkKBfndmUiIiJFXq7BGlgGWMCc5RzrnXJExHX+QdD3WajeAabfDhM6woAJUK+325WJiIgUaXkJ1jUKvAoRKXoaD4QqzeDj4fD+5ZBwK3R/GPwC3K5MRESkSMo1WFtrNxdGISJSBEXVghu+d/qtF78EWxbD4DchoprblYmIiBQ5unhRRM7OLxAufhqGvAWJ6+DVjrDmS7erEhERKXIUrEUkbxpdCqPmQUQN+PAqmDEG0k64XZWIiEiRoWAtInkXWRNu+A4uGA0/vgKTLoIDm9yuSkREpEhQsBaR/PELhN5PwmXvwL71MKET/D7d7apERERcl+9gbYwpb4y5wBgTWBAFiUgx0bA/jJ7vXOD40TXw9f2QdtztqkRERFyT52BtjAk1xnwE7AF+AKI9+ycYY8YWTHkiUqRFVIfrv4W2N8PSV+GNnrB/g9tViYiIuCI/I9ZP4oTpFsCxbPu/BAZ4sygRKUb8AqDXf+Hy9+DARni1M/w2ze2qRERECl1+gnV/4E5r7UpOXWlxDVDTm0WJSDFUvw+MWgDl68LU6+CreyE1xe2qRERECk1+gnUEsC+H/aFAunfKEZFiLaIaDJ/hrNL402vwxoXOBY5SoDIybO4niYhIgctPsP4JZ9Q6U+ZP8lE4PdciIk5ryEWPwRUfwMEt8Gpnqmz92u2qSpzN+47w8py/6PX8fJo+8h2HjqW6XZKISKmX65Lm2TwIfGuMaeR53N2e7TZAp4IoTkSKsXq9YfRC+Hg4zZfezaN+3TFpbXD+yCXnYtehFL5cvYMvVu9k1daDAFQOCyL5eBqHjqYSHuzvboEiIqVcnkesrbU/AO2AAGA90B3YASRYa1cUTHkiUqyVi4XhM1hf9wau9ptF3Gf9IfEvt6sqVvYlH+fdJZsZ+upiEp6YxaNfrSE9I4MHL67PojHduO+iem6XKCIiHvkZscZa+wtwXQHVIiIlka8/fzS5j0d/jeT15NdgYmfo9wI0GVygL5uansHupBRiIsoU6OsUhKSUVL77bTfTV+1g0V+JpGdYalcM4a4edenbtAo1K4S4XaKIiOQgz8HaGPMz8A7wnrV2V8GVJCIl0ZyM5mwa8h215t0Gn9wAmxZAryfAP9irr3PoaCrv/7SFt37YxO6kFBY/0J1KYUFefY2CcOxEOrPW7mb6yh3M/WMvJ9IziIkIZlSnmvRrVpX6lUMxxrhdpoiInEV+RqxnALcCTxpj5uKE7E+ttckFUZiIlDxpIVVh2Fcw+1FY9DxsWwZDJkP5Ouf93BsTj/Dmoo18vHwbR0+kE10umAwLh1PSqBR23k9fII6npbPgz0Smr9rBzDW7OXoinYqhgVzVNo7+zaoSH1tOYVpEpBjJc7C21j4IPGiM6QBcCTwDvGKMmQ68Y63VZf8ikjtff7jwEajeAT4d6Swo0+95aHpZvp/KWsuSDft5Y+FGZq3djb+PD/3jq3J9+xr8tTeZ29//2fv1n6e09AyWbNjP9FXb+ebXXSSlpFGujD+XxEfTv1lV2tSIxNdHYVpEpDjKV481gLV2IbDQGHM70Av4D/AF4Ovl2kSkJKtzoTNryCc3wKcjPK0hT0JA7j3RJ9Iy+HL1Dt5YuJHfdiQRWTaA27rW5uqEalQMddo+/tpbdP6YlpFhWbHlANNX7eDrX3aSmHyCsgG+XNSoMv2aVaVDnfL4++Zn9lMRESmK8h2sAYwxsTij1lcBjYCF3ixKREqJ8Gi47kuY8xgsfNbTGvIWVKib4+n7j5zgvR838/bizew5fJw6FUN4YmATLm0eTZB/0frd3lrLbzuSmL5qB1+u2sGOQykE+vnQvUFF+jWtStf6FYtczSIicn7yc/FiBDAEJ0y3B/4ApgDvWmu3FEx5IlLi+fpBj4ehenunNWRiZ+jzLMRfkXXKX3uSmbRoI58s38bxtAw61a3A00Nq0KlO+SLXg/zXnsNMX+nMNb0x8Qh+PoZOdStwX6969GhQidAgzTUtIlJS5WfEeheQCHwI3KW5q0XEq2r38LSG3AjTRmM3LeCHemN4bcku5v6xlwA/HwY2j+b6DjWoW6loLTJz5HgaX6zawfs/bWXV1oMYAwk1oxjZqSa9GlUmomyA2yWKiEghyE+w7gfMtNZmFFQxIlLKhVUl5crP2DD139RfOZHyK+ZxxP9e7r6wA1ddEEdUSKDbFWax1rJ62yE++GkL01fu4MiJdGpXDOFffRrQv1lVKhaDKf5ERMS78jMryHcFWYiIlG6JnhUG312ymcTkzlwRFc3Dqc/zkX0QEzkOQs5/Sj5vOHQslc9Xbuf9pVtZszOJIH8f+jSpypUXxNIiLqLItaaIiEjhOWuwNsasBjpbaw8YY34B7JnOtdY29XZxIlLy/bHrMG8s3MC0lTs4kZZBt/oVuaFDDdrVuhiTPNRpDfn8Zti0EPqMg4CyhV6jtZZlmw/w/tItfP3LTlJSM2hYJYz/XNKI/vHRhAerb1pERHIfsf4EOJ5t+4zBWkQkrzIyLPPW7WXSwo0sWJdIkL8Pl7WKYXj7GtTKvlx3aGW49nOY9xTMexK2L3cWlKnUsFDq3H/kBJ+u2MYHP23lrz3JlA3wZUDzGK5oE0uT6HCNTouIyCnOGqyttY9k2x5b4NWISIl2LDWdKT9uZtLCjazfe4RKYYHcd1E9rmwTd+YL/Hx8oesDUC0BPhkBr3WDi5+G5ldDAQTbjAzL4g37eH/pFr77bTcn0jOIjy3Hk4Oa0LdpVcoGntMspSIiUgrkZ7q92cBAa+3B0/aHAdOstd28XJuIlDCXT1xMSmoGjaPDeH5oPBc3qUKAXx4XRqnZxZk15NMbYfqtntaQZyAwJNeH5sWepBSmLt/GR8u2snnfUcKC/LjygjgubxNL/cpFdE10EREpUvIz9NIFyGlIKQjo6JVqRKREqhIehI+BjnUqcGOHGrSpEXlubRShleCaaTB/HMz9L+xY4WkNaXROdaVnWOb/uZf3l25h1to9pGdYLqgRyZ096tC7cRUt4CIiIvmSa7A2xrTIdrepMWZ/tvu+wEXAdm8XJiIlR/O4CP567GJ8fLzQuuHjC13+4WkNudFpDen9FLS4Ns+tIdsPHuOjn7YyddlWdhxKIapsADd2qMHQ1rHUrOCdEXARESl98jJivQznokUL5DTl3jHgNm8WJSIlj1dCdXY1OnlaQ0bAF7fDpgXQ9zkIzHnxmNT0DGat2cMHP21h3p97AehQuzz/6tuQHg0q5b0lRURE5AzyEqxrAAbYALQB9mY7dgLYY61NL4DaRETOLqQiXP0pLHgW5j4OO352WkMqN8k6Zcv+I3yyYhsfL9/G3sPHqRQWyK1da3NZq1hiI8u4V7uIiJQ4uQZra+1mz6aGc0Sk6PHxhc73Oa0hH98Ar3WH3k+CX08Arp+8DB8D3epX5PLWcXSpVwE/X/04K0627j/K7LV72Jd8nLsurKtpDkWkyMptgZiBwBfW2lTP9hlZaz/1amUiIvlRvYPTGvLZSPjyTrrWuYSOsdfQpn51hrSKpXK4lhgvLtLSM1ix5SCz1u5m9po9rNuTnHXsyguq6XspIkVWbiPWHwOVgT2e7TOxOBcyioi4J6QCXPUJLHyW0DmP8U7EL9BwMiiIFXkHj55g3p97mbVmD/P+3MuhY6n4+RguqBnJ5W3i2H/kOC/PWe92mSIiZ5XbAjE+OW2LiBRZPj7Q6V6o1s5pDXn9Quj1OLS6oUAWlJFzY61l3Z5kZq3Zw5y1e1i2eT8ZFqLKBnBhw0p0r1+RDnXKExrkLBf//tItLlcsIpI7LSEmIiVTtXYwegF8Ngq+usdZUKbfCxAU7nZlpVZKajpLNuxj9to9zF67h20HjgHQqGoYt3atTdf6FWkWU877M8iIiBSS/Ky8eBlw0Fr7nef+Q8BI4DdgmLV2Z8GUKCJyjsqWhyunwg8vwKz/wI6VzqwhVeNdLqz02J2UkhWkF65L5FhqOkH+PnSoXYGbu9Sma/0KVAkPdrtMERGvyM+I9VjgTshaNOZB4CGgF/AMcKWXaxMROX8+PtDhLohLgI+vhzcuhJ6PQZsRag0pABkZltXbDzF7zW5m/7GHX7cnARBdLpjBLWPo1qAiCTWjtKqliJRI+QnW1YA/PNsDgGnW2qeMMd8B33q9MhERb4prC6MWwLSbYMZ9zoIyl7yk1hAvOJySysJ1icxeu4c5f+whMfkEPgZaxEVwf696dK9fibqVQjRNnoiUePkJ1ilA5pJm3YFJnu1D2faLiBRdZaPgig9g8UswcyzsWg2D34ToFm5XVuxsSjyS1eLx48Z9pKZbwoL86FyvIt3rV6Rz3QpElA1wu0wRkUKVn2C9AHjGGLMQaAUM9uyvC2z1dmEiIgXCxwfa3w6xF3haQ3pCz0fhglFqDTmLzFk8vv5lJzN+2cUfuw8DULtiCNe3r0G3+hVpWS1Ci++ISKmWn2B9K/AKTqAeba3d4dnfG7WCiEhxE3eBM2vItJvhm394WkNehuBybldWZFhr+X1nEjN+2cWMX3eyfu8RjIHW1SJ5qG9DejSoRFyUloUXEcmU52Btrd0G9Mth/53eLEhEpNCUiYQr3ofFL8PMh+HVjjB4MsS0dLsy11hrWb3tEDN+dcL05n1H8THQtmYUw9rX4KJGlagYqgV3RERyku95rI0x3YCGOKst/m6tneP1qkRECosx0O5WT2vIcJh0EVz4f9D2plLTGpKRYfl56wHPyPQuth88hp+PoV3t8tzUuRYXNqxEVEig22UWmuNp6SzffIAF6xI5dCyVRy9prLm1RSRP8jOPdTTwGdASyGwDqWqMWQYMyNYaIiJS/MS2hlHz4fNb4NsHnAVlLn0ZgiPcrqxApGdYlm3anzUyvTvpOAG+PnSsU567LqxLjwYVKVemdFx8aK1lQ+IR5v+5lwXrElmyYR9HT6RnHb+zRx2N0otInuRnxHo8kA7UttZuBDDG1ATe9RwbfJbHiogUfWUi4fL3YMkr8P1DMKETDHkTYlq5XZlXpKVn8OPG/Xz9y06+/W0XicknCPTzoUu9CvRuXIVuDSoS5llCvKQ7dDSVResTWbBuL/P/TGT7QWcVyOpRZRjUIoaOdcqzIfEIT8xY63KlIlKc5CdYXwh0yQzVANbaDcaY24FZXq9MRMQNxkDCzZ7WkGFOa0iPRyDhlmLZGnIiLYMf1icy45ddfPf7Lg4cTSXY35du9SvSu0llutarSNnAfHcFFjtp6Rms2naQeX86YXrV1oNkWAgJ9KNdrShu6lKLTnUqnHIx5rtLNrtYsYgUR974aZrhhecQESlaYlo6C8p8fgt8909Pa8j/nFHtIi4lNZ2F6xKZ8esuvv99F0kpaYQE+tG9QUV6N65C57oVCA4o+Ssfbt1/lPnr9rLgz0QWrU/kcEoaxkDTmHLc2rU2HetWID62HP6aIlBEvCQ/wXoWMN4Yc4W1diuAMSYOeAGNWItISRRcDoa+Cz++Ct/9CyZ0dFpDYtu4XdnfHEtN5xtPv/SsNXtIPp5GWJAfFzaszMVNKtO+dvkSv4x48vE0lqzf54TpdYlsTDwCQJXwIC5uXIVOdSvQvnZUqekdF5HCl59gfTvwObDBGLMDZ1aQaGC155iISMljDLQd7VzcOHU4vNkbuj8ECbc5i80UEX1fXEBquiWijD99m1ahd5MqJNSMIsCv6NTobRkZlt92JDF/3V7m/7mXFVsOkJpuCfL3oW3NKK5pW41OdctTq4KWUxeRwpGfeay3Ai2MMT2ABoDBmW5vZkEVJyJSZES3dGYNmX6bc2HjpkVw6SvOMukuqlc5lHqVQmldI4KLG1ehTY3IEr364a6kFOeCw3WJLPorkf1HTgDQsEoY13eoQac6FWhVPYJAv5I9Oi8iRVOegrUxZghwKeAPzLTWvliQRYmIFEnB5eCyt+Gn1+HbBz0LykyCuLauldQ4Opxv7+rk2usXtktfXgRA+ZAAOtetQKe65Wlfu7ymwxORIiHXYG2MGQlMANYBKcAgY0wNa+0DBV2ciEiRYwy0GQExrWHqMHjzYuj+b2h3R5FqDSlpLqgRSZ8mVWgSE07HOuVpUDlMi7aISJGTl/8FbgMes9bWs9Y2A24Abi3YskREiriq8TBqHjToBzPHwnuXwZFEt6sqsWpWCOHlq1owunMtGlUNV6gWkSIpL8G6JvBmtvvvAIHGmMoFU5KISDERFA5DJkOfZ2DjfGfWkM0/uF2ViIi4JC/BOhhIzrxjrU0HjgNlzvgIEZHSwhhofSPc+D34B8HkvjB/HGRoin8RkdImr7OCjDbGJGe77wfcYIzZl7nDWvusVysTESlOqjSDkfPgiztg9n9g8yIYMBFCKrhdmYiIFJK8BOstwPDT9u0Crsx23wIK1iJSugWFObOE1OgIM8bAhA4w+A2o3sHtykREpBDk2gpira1ura2Ry63muRZgjHnOGLPAGPPCafvHGmNWGWPmGmPuPtfnFxEpVMZAq+thxCwIDIG3+sG8pyEj3e3KRESkgLk6N5QxpgVQ1lrbEQgwxrQ+7ZR7rLVd1GYiIsVO5SYwci40HgRzHoV3B0LyHrerkmLAWut2CSJyjs4arI0xV5s8rgNrjKlujOmYz9dPADJXbpwJnL7KwpPGmJnGmPh8Pq+IiPsCQ2Hga9BvPGxZ4rSGbJzvdlVSxOxOSmHaz9v5x8er6fjUbFo/NouUVP2FQ6Q4yq3H+nrgIWPMZGA68JvN9qu0MSYS6ABcDXTxnJ8f5YD1nu1DQKNsx8Zba8caY+oAk4C/hXbP4jUjAeLi4vL50iIihcAYaHmdsyT61GHw9iXQeQx0uhd8tOx2abQv+ThLNuxn8YZEfli/jw17jwAQHuxPVNkAtiYf4+iJdIL89fkQKW7OGqyttd2MMX2A24H/ACnGmD04KzBGABWAPTjzXN9ird2bz9c/CIR5tsM89zNfe7/n33VnGjS31k4EJgK0atVKfzsTkaKrcmOnNeSru2Hu47B5IQx8HUIruV2ZFLBDR1P5ceM+fli/jyUb9rF212EAygb40qZGJFe0jiOhVhQNqoTx7pLNPDz9N5crFpFzleusINbar4CvjDHlcUanq+HMbZ0I/Az8bK091wlbFwOjgI+AHsDkzAPGmDBrbZLndfM6LaCISNEVGAIDXoXqHeHr+5zWkEGvQc0ublcmXpR8PI2fNu1n8fp9LF6/j193HMJaCPL3oXX1SPo1q0pCrSiaRIfj7+vqpU4i4mV5DqzW2kRgmjdf3Fq7whiTYoxZAKyy1i41xrxorb0NeNoY0xinD3yMN19XRMQ1xkCLa7K1hlwKne+Hzv9Qa0gxdexEOss3H2DxhkQWr9/Hqm2HSM+wBPj60DyuHHd0r0O7WuVpFhtOoJ++xyIlmesjwdbaO067f5vn31HuVCQiUggqNYSRc+Cre2Hek85S6INeh9DKblcmuTiels7KLQdZvMFp71i55SAn0jPw9TE0iwnnps61SKgVRctqEeqTFillXA/WIiKlVkBZGPCKs4DMV/c4rSEDJ0Ktbm5XJtmkZ1hWbDmQ1dqxbPN+UlIzMAYaVw1nePvqtK0VRevqkYQEFp//VtMzLGt3JfHTxv2kpltGdDrnJSlylZFhWb83mRVbDrBi80FWbDmABWbc0VHtMFKiFJ+fACIiJVXzqzytIdfBOwOdGUM6jwFf/YguCrqOm0tKqnMpUf3KoVzRJo6EmlFcUCOK8DL+LleXdymp6azedoifNu1n6cb9rNh8gMPH07KOX9Y6lvBg73w9h46lsmqrE6BXbDnIyi0HSEpxXis82J+wYD+27j9GSmq6grWUKPqpLSJSFFSsDyNmw9f3w/ynPa0hb0BYFbcrK7WaxoQTH1uOxtFhJNQsT9uakUSFBLpdVp4dOpbKis0HWLppP8s27WfV1kOcSHd+QahbKYR+8VVpUz2StbsOM2HeejjHubVyGo3+a28y1jqXFNSrFEqfplVoHhdBi7gIapYvy6RFG3n0qzVe/GpFigYFaxGRoiKgLFz6sqc15G5Pa8irULuH25WVSk1jyjHtlvZul5Fnu5NSWLrRCdFLNx1g7a4krAU/H0Pj6HCGta9O6+qRtKoWQUTZgKzHTVq4MV+vk9todPO4cvRrVpUWcRE0iw0nNKj4jOqLnK98BWtjzAVAd6Aip63aaK293Yt1iYiUXvFXQHQLZ9aQdwdBx3ugy4NqDZEs1lo2JB5xQvTGA/y0aT9b9h8FoEyALy3iIrize11a14igeWwEwQHndhHluYxG+/jkacFmkRIpzz+ljTH3Ak8BfwE7OPWPRlqcRUTEmyrUgxtnwTf/gAXPwObFzqwh4dFuVyYuSEvPYM3OwyzdtJ+fNu5n2eb9JCafACCybACtq0dwbUI1WlePpGHVsHPuW9ZotMj5yc/wxx3A7dbalwqqGBERySagDPR/0VlQ5os7T84aUudCtyuTAnYsNZ3F6/fx06b9/LTJudDwyIl0AGIjg+lUpwKta0TSunoktSqU5UwrFOfHgP8tYuO+IxqNFjkP+QnWYcDXBVWIiIicQdPLoGpz+Og6mDIY2t8J3f4FvhotLKk6PTWH9AybFXAHtojxBOkIqoQHe/W16lYKpWp4EHFRZbi0ebRGo0XOQ36C9ftAL+B/BVSLiIicSfk6MGIWfDMGFj0PWxbD4EkQHuN2ZeJFratH0rluBRpUCaNNjQhaxkUW+JR+HeqU54cHuhfoa4iUFvkJ1luBR4wx7YHVQGr2g9baZ71ZmIiInMY/GPq94GkNucNpDRnwKtS9yO3KxEsaVg3jrevbuF2GiJyj/ATrG4FkoJ3nlp0FFKxFRApDk8FQJd6ZNeS9y6Dd7dD9IbWGiIi4LM/B2lpboyALERGRfChfG26cCd8+AD+Mhy1LnNaQcrFuVyYiUmqd03w8xpgQY0xZbxcjIiL54B8EfZ+DwW/CnjVOa8gfM9yuSkSk1MpXsDbG3GKM2QIcApKMMZuNMTcXTGkiIpInjQfCqHlQLg7evxy+/SeknXC7KhGRUifPwdoY8yDwBPAG0NNzexN4whgzpmDKExGRPImqBTd8D61HwOKX4M3ecHCL21WJiJQq+RmxHg2MtNY+Yq2d5bmNBW7y3ERExE3+QdBnHAx5CxL/dFpD1n7ldlUiIqVGfoJ1ReCnHPYvBSp5pxwRETlvjS51WkMiqsMHV8I3D6g1RESkEOQnWP8JXJnD/iuBP7xTjoiIeEVkTac1pM0oWPI/mHQRHNjkdlUiIiVafuaxHgt8ZIzpBCzCmbu6A9AZGOL90kRE5Lz4BcLFT0H1DvD5rTChE1z6MjTo53ZlIiIlUp5HrK21nwIXALuAvkB/z3Yba+20AqlORETOX8P+TmtIVC348GqY8Q9IO+52VSIiJU5+Rqyx1i4Hri6gWkREpKBE1oDrv4WZDzutIVuWwJDJzn4REfGKs45YG2Mis2+f7VbwpYqIyHnxC4Be/4WhU+DARni1E/w2ze2qRERKjNxaQfYaYyp6thOBvTncMveLiEhx0KAvjFoA5evA1Ovg6/vUGiIi4gW5tYJ0A/Z7trsWcC0iIlJYIqrB8G9g1iPOgjJbf3SWRo+q5XZlIiLF1lmDtbV2Xk7bIiJSAvgFwEWPQbX2MO0meLUz9B/vLJEuIiL5lp8lzRsaY+plu3+hMeZdY8wDxhjfgilPREQKXP2LYfQCqFgfPh4OX94NqSluVyUiUuzkZ4GYN4DmAMaYGOBzIBK4BXjU+6WJiEihKRcHw2dAu9tg2RvwRg/Yt97tqkREipX8BOsGwArP9hDgR2vtxcA1wBXeLkxERAqZrz/0fBSu+BAObXNmDfnlY7erEhEpNvITrH2BE57t7sDXnu31QCVvFiUiIi6q1wtGL4RKjeCTG+CLOyH1mNtViYgUefkJ1r8CNxljOuIE6288+6NxptwTEZGSIjwGhn0F7e+A5W/C6z0gcZ3bVYmIFGn5Cdb/AEYAc4H3rbW/ePb3B5Z6uS4REXGbrz9c+H9w5VRI2uHMGrL6I7erEhEpsvIcrK2184EKQHlr7fXZDr0K3OTtwkREpIio29NpDancBD4dAdNvU2uIiEgOclsg5hTW2nTgwGn7NnmzIBERKYLCo53WkDmPwcJnYdtyGDIZKtR1uzKRHJ1Iy+DP3Yf5dfshVm8/xK/bD+HnY5g6uh2+Psbt8qSEOmuwNsZMB6621iZ5ts/IWtvfq5WJiEjR4usHPR52FpT5bCRM7AJ9n4Vml7tdmZRyqekZ/LHLCdG/eEL0mp2HOZGeAUBokB8hgX7sPJTCibQMggO0/IYUjNxGrPcBNtu2iIiUdnV6OK0hH98An42CTQug99MQUMbtyqQUSE3PNhK9zROidx3mRNrJEN0kOpzh7avTODqcpjHhxEWW4dX5G3hixlqXq5eSLrclzYfntC0iIqVcWFW47guY+19Y8MzJ1pCK9d2uTEqQ7CH6l+2H+GV7Emt2Jp0SohtXDWdYu+o0iQ6nSbQTon3U6iEuyXOPtTGmMuBnrd122v4YINVau9vbxYmISBHm6wfd/w3V2sGnI+G1rtDnGYi/0u3KpBhKTc9g3e5kT0/0wb+H6EA/GkWHMaydMxLdJDqcagrRUsTk5+LFd4CPgNdO238RMBTo6a2iRESkGKnd3WkN+XQETLsJNi2Ei5+GgLJuVyZF3KcrtvPXnmR+2X6INTuTOO4J0SGBfjSODuO6hGpZIbp6VNliE6KTj6exdmcSv+9MItDPh6Gt49wuSQpJfoJ1a+DWHPYvAJ72TjkiIlIshVWBaz+HeU/CvKdge2ZrSAO3K5MiKNDPme334em/ERLoR6OqYVzTthpNYopXiLbWsisphd93JDk3T5jevO/oKecNaB5DgF9+lg45d8fT0snIQBdouiQ/wdoPCMxhf9AZ9ouISGni4wtdH4S4BGf0emJX6DMO4q8CU/RDkhSeAS1iqBQWRK2KIdQoJiE6NT2D9XuTTwnRa3YmceBoatY51aPK0KhqGINbxNCwahg/rN/HGws3YrPmgfAeay3bDhzjj12H+WP3YdbuOswfu5LYsPcI4cH+/PTPHsXifS1p8hOsf8RZCOb0xWBuAX7yWkUiIlK81erqtIZ8ciN8fgtsXOD0XgeGuF2ZFBEhgX70bFTZ7TLOKCkllTWZI9A7klizK4k/dyVnTd8X6OdD/cqh9GpcmQZVwmhYJYz6VcIICTw1Vq3dddgr9Rw8esITnE8G6D93J5N8PC3rnJiIYOpXDiUk0I8VWw6SYS0+KFgXtvwE638Cs40xzYBZnn3dgOZAD28XJiIixVhoZac1ZP7TMPcJ2LHCaQ2p1MjtykSyWGvZfvDYyTYOT4jeuv/kyqJRZQNoWDWM4e2r07CqE6JrlC+Ln6/3WzuOp6Xz155kZxTaE6LX7kpid9LxrHPKlfGnXqVQBrWIpl7lMOpVDqVupRBCg/wBeHHWOlZsOej12iRv8hysrbVLjDEJwP3AQMAAK4CbrbWrCqg+EREprnx8ocsYpzXkkxvhtW7ORY3Nr1FriLjm05+3sX7PEX7feYjfdySRlOKM+hoDNaLK0jSmHJe3jqNh1TAaVQmjQmggxsuf14wMp41j7a4kJ0DvdoL0xsQjpGc4bSMBvj7UrhhC+1rlqVc5lHqVQ6lfOYxKYd6vR7wnv0uarwKuKqBaRESkJKrZ+eSsIdNvc1pD+j4LgaFuVyalSOYFk//87FeC/H2oXzmMvs2q0rBKGA2rhlG/cihlAvIVi/Lsxw372bA3mT92H2bNzsOs232YIyfSs47HRgZTv3IYvRtX9gToUKpHFcyouBSsfH2CjDGVgGuAmsBD1tpEY0x7YIe1dmNBFCgiIiVAaCW45jNnMZm5//W0hrwFlRu7XZmUEkNaxRIbUYYaFcpSPaosvoVwYZ+f5zWunbQUgIgy/tSrHMqQVrFZo9B1K4X+rTdbiq/8LBDTEqe3eiPQCBgHJAIXAnUBrQggIiJn5uMLne8/2Rryenfo9QS0HKbWEClwIYF+9GhYqVBfc2CLGIIDfKkWVZYGlUMLpK1Eipb8/I1hHPCCtbY5cDzb/m+B9l6tSkRESq4aHZ3WkLgE+PJO+OQGSElyuyoRr6sQGsi1CdXpXLcCFcOCFKpLgfwE65bAWzns3wkU7q+AIiJSvIVUgKs/hW7/ht8+g4ldYOdqt6sSETkv+QnWx4CIHPbXB/Z4pxwRESk1fHyg071w3ZeQehRe7wE/vQ7W+4tpiIgUhvwE68+Bh40xmassWmNMdeBJ4BNvFyYiIqVE9fZOa0j1DvDVPfDxcLWGiEixlJ9gfS8QCewFygALgb+Ag8C/vF6ZiIiUHmXLw1UfQ/eH4ffp8Gon2KklEkSkeMlPsE4DugCXAv8AXgB6WWs7W2uPeL80EREpVXx8oOPdMOwrSDvutIYsfU2tISJSbORpuj1jjC9wCGhmrZ0NzC7QqkREpPSqluC0hkwbDV/fC5sWQP8XISjc7cpERM4qTyPW1tp0YDMQULDliIiIAGWj4IoPoccjsOZLpzVk+wq3qxIROav8tIL8B3jCGFO+oIoRERHJ4uMDHe6E4TMgPQ3e6Ak/vqrWEBEpsvJ78WIHYLsxZr0xZnX2WwHVJyIipV3cBTB6AdTqBjPuh4+ugWMH3a5KRORv8rM4/SeAhglERKTwlYmEKz6AxS/BrEec1pAhb0J0S7crExHJkudgba0dW4B1iIiInJ2PD7S/3VkK/ePh8MZF0PM/cMFo0FLRIlIE5NoKYowpY4x52Riz3RizxxjznvqsRUTENbGtYdR8qHMhfDMGPrwajh1wuyoRkTz1WD8CDAO+Aj4ALgReKcCaREREzq5MJFz+Hlz0OPz5DUzoBNuWuV2ViJRyeQnWA4EbrLUjrbW3A32ASz1zW4uIiLjDGEi4Ba7/1rk/6SL44SXNGiIirslLsI4FFmTesdYuxVmFsWpBFSUiIpJnMa1g9Hyo2wu++ye8fwUc3e92VSJSCuUlWPsCJ07bl0b+ZhQREREpOMERMPRd6PUk/DXTmTVk61K3qxKRUiYv4dgA7xpjjmfbFwS8Zow5mrnDWtvf28WJiIjkmTHQdrRzcePU4fBmb+j+MCTc6swoIiJSwPLyk+YtYAewL9vtXWDraftERETcF93SmTWkXm/4/t/w/uVqDRGRQpHriLW1dnhhFCIiIuI1weXgsndg6WtO3/WEDjD4TWcVRxGRAqK/jYmISMlkDFwwEm74Dnz9ndaQhc9BRobblYlICaVgLSIiJVvV5k5rSIN+MHMsvHcZHFEHo4h4n4K1iIiUfEHhMGQyXDwONs5zWkM2L3a7KhEpYRSsRUSkdDAG2oyAG74Hv0CY3AcWPKPWEBHxGgVrEREpXarGO60hDS+BWf8HUwbDkUS3qxKREkDBWkRESp+gMBg8Cfo8C5sWOq0hmxa5XZWIFHMK1iIiUjoZA61vgBtngn8ZeKsvzH9arSEics4UrEVEpHSr0hRGzYNGA2H2o/DuQEje63ZVIlIMKViLiIgEhsKg16HfC7BlsdMasnGB21WJSDGjYC0iIgJOa0jLYXDjLAgMgbf7w9wnISPd7cpEpJhQsBYREcmucmMYOQ+aDIG5j8M7A+DwbrerEpFiQMFaRETkdIEhMOBV6P8SbP3RaQ3ZMM/tqkSkiFOwFhERyYkx0OIaGDEbgsvB25fAnP+qNUREzkjBWkRE5GwqNYIRc6DZ5TDvCSdgH97ldlUiUgQpWIuIiOQmMAQGTIBL/gfbljmtIevnuF2ViBQxCtYiIiJ51fwqGDkHgiOdixpnP6bWEBHJomAtIiKSHxUbOOE6/kqY/xS81R+SdrpdlYgUAQrWIiIi+RVQFi79H1z6CuxY4bSG/DXL7apExGUK1iIiIucq/krnwsayFeDdQTDr/yA9ze2qRMQlCtYiIiLno2J9Z0q+5lfDgmfgrX6QtMPtqkTEBQrWIiIi5yugDFzyEgyYCDtXOa0h62a6XZWIFDIFaxEREW9pNhRGzoWQyjBlEMwcq9YQkVJEwVpERMSbKtSFEbOgxXWw8DmY3AcObXO7KhEpBArWIiIi3uYfDP3Hw8DXYfevMKEjLH8LDm51uzIRKUB+bhcgIiJSYjUdAlWbw9Rh8MXtzr6wGIhrC9USIC4BKjQAH41ziZQECtYiIiIFqXxtGDXPGbnesgS2LIZNC+HXj53jQeEQe4ETsuMSnCDuH+RuzSJyTgo1WBtjngNaASustXdk2/8w0Mtz91/W2lnGmGHAA8BOYKm19v7CrFVERMRrfHyhSjPndsEosBYObDoZtLcsgXXfOef6BkDVFidHtGPbQHCEq+WLSN4UWrA2xrQAylprOxpjXjHGtLbW/uQ5/La19hFjTDlgOpC5fNXT1trXC6tGERGRQmEMRNZwbvFXOPuOJMLWH52gvXkx/PCic/EjBio2dNpH4hKcf8vFulq+iOSsMEesE4DMST1nAm2BnwCstRs9+48DNttj7jTGXAs8Yq3VWrEiIlJylS0P9fs4N4ATR2H7cs+o9g+w+kNY9oZzLDzWE7Tbqk9bpAgpzGBdDljv2T4ENMrhnLHAq57tacDbQBTwnTGmlbU2PfvJxpiRwEiAuLg4rxcsIiLimoAyUKOjcwNnPuw9vzmj2VsWw8b58MtU51hQOMRmC9rRLcAv0L3aRUqpwgzWB4Ewz3aY534WY8wAIMpa+x6AtTbz+F5jzJ9AJeCUNWKttROBiQCtWrXKPtItIiJSsvj6nezTbjs6W5/24mx92t96zg10wnVcW4hr5+nTLudm9VKCWGs5cDSVbQeOsnX/MbYeOEpkmQAua60WpcIM1ouBUcBHQA9gcuYBY0xT4BagT7Z9YdbaJGNMMFAH2FuItYqIiBRtp/RpX+nsO5J46gWROfVpV2vn/Bse42r5UrQdOZ7G1szgvP9o1va2A0fZduAYycdPXVHUGBSsKcRgba1dYYxJMcYsAFZZa5caY1601t4GPI0zIv2tMeaQtfYS4C5jTC+cRWyesNamFlatIiIixVLZ8tCgr3MDT5/2spNhO8c+7cz5tOurT7sUOZGWwfaDp4bmrQeOsm3/UbYeOMb+IydOOT/Y35fYyGBiI8rQtmYUMRHBxEaWITaiDNNWbmfi/A0ufSVFS6FOt5d9ij3P/ds8/16Uw7mPAI8UUmkiIiIlT0AZqNHJuYHTp519Pu1T+rTLnXpBZNXm6tMuxtKtZU9WcD4ZoLd5AvSupBRstiZaf19D1XJOcL6oURgxEWU8wdkJ0FFlAzDG5Pha3/62q5C+qqJPC8SIiIiUFr5+UDXeuWX1aW88GbQ3L4Y/v/GcGwjRLU8GbfVpFyuNH/6W1PSTydkYqBwWRGxEGRJqRREbUebkqHNkGSqHBeHrk3NwlrxTsBYRESmtjIHIms4txz7txfDDeFj4LGCgUqNT20fCo10tX/6uR8NKbEw8QsWwoKzWjdjIMlQtF0Sgn6/b5ZV4CtYiIiJy0t/6tI8482lnTvO36gP4ybN2W3jcyfaRau2gfD31abusQZUwnh0a73YZpZaCtYiIiJxZQNkz9Gl7gvaGufDLR86xU/q02zktJ+rTllJEwVpERETy7pQ+7ZtO9mlvzjafdk592tXaQUxr9WlLiaZgLSIiIucue59286ucfcl7YeuSk73a6tOWUkLBWkRERLwrpAI06OfcwOnT3pZtPu2V75/ap10t4WTYVp+2FGMK1iIiIlKwAspCzc7ODTx92r+cDNrr5ziL1wAER0Bs9vm049WnLcWGgrWIiIgULl8/ZwGaqs1P9mnv33DqNH9/znDO9Qv6+3zaQeHu1i9yBgrWIiIi4i5jIKqWczu9TzvzosiFz4N9BqdPu/Gp0/yFVXWzepEsCtYiIiJS9JyxT9sTtFe+Bz+95hwrF3fyYsi4BChfV33a4goFaxERESn6ztSnnTmivX723/u0q3mCdpV48AtwrXQpPRSsRUREpPjJ3qedcHO2Pu3s82mf3qftCdqxrdWnLQVCwVpERESKv1P6tK929iXv8VwQ6bkocuFzYMeR1aedfZo/9WmLFyhYi4iISMkUUhEa9nduAMeTYbtnPu3NP8DP78LSic6xctU8I9qeoF2hnhPWRfJBwVpERERKh8AQqNnFuQGkp8KuzPm0f4D1s2D1B86x4MiTM4+oT1vySMFaRERESidff4hu4dxO79POvCjyj6+dc/2CILrVafNph7lbvxQ5CtYiIiIikEuf9uJT+7SND1RqdOo0f2FV3K1fXKdgLSIiInImZ+rTzhzRzqlPu1q2+bTVp12qKFiLiIiI5FWOfdqrT45q/zUzhz7tzPm0m6lPu4RTsBYRERE5V77+zhzZ0S0h4RanT3vf+pNzaZ/Spx0MMa1OXhQZoz7tkkbBWkRERMRbjIHytZ1bi2ucfYd3w9YlJ6f5W/AM2AxPn3bjU6f5U592saZgLSIiIlKQQitBw0ucG8Dxw7Bt2clp/n5+B5a+6hyLqJ4taLeD8nVKbZ/2ibQM9iYfZ09SCnsOH2fP4ePszbadnJLGR6MT3C7zFArWIiIiIoUpMBRqdXVucGqf9uYfYN33sOp951hw5MmgXa0dVG5a7Pu0j55IY0/ScU9ATjlle+/h4577KRw4mvq3xxoDUWUDqRgaSMWwQFLTM/D39XHhq8iZgrWIiIiIm87Yp/1Dtj7tr5xzT+nTToCY1kWmT/vQ0VQnKP8tMDujznszR5qPp/3tsf6+hgohgVQICyIuqgytqkdQMTSIimGeEO3ZjiobgF8RCtKnU7AWERERKUpO6dO+1tl3ePepF0Tm1KedOc1faOVCLxeg2f9997djwf6+WeG4QZUwOtUN9NwPyhp1rhgaRLlgf3x8in/Li7HWul2DV7Rq1couW7bM7TJERERECt7xw7Dtp5NBe9sySD3qHMvq086cT7tg+7Q37zvCR8u2ElEmgArZRpcrhgYSEuiHKYE94saY5dbaVn/br2AtIiIiUsylp8LO1SdXiNyyBI4mOsfKRJ0680iVZk77iZyzMwVrtYKIiIiIFHe+/hDT0rm1u9XTp/3XyZC9+QdY+6Vzblaftidsx7ZxLqiU86ZgLSIiIlLSGOO0gJSvk61Pe5endcQzzd+CcSf7tCs3OXVUu5D7tEsKtYKIiIiIlEaZfdqbF5/s00475hyLqHHqNH9RtUvtfNo5USuIiIiIiJwUGAq1ujk3yNan7Znmb923sOo959gpfdrtoEpT9WnnQMFaRERERE7r077tZJ/25mzzaZ/ep12tnRO2Y1qrTxsFaxERERHJSfY+7ZbXOfsO7zp1Pu35T5/Wp90uW592JXfrd4F6rEVERETk3KQk/X0+7ex92tWyBe0S1KetHmsRERER8a6gMKjd3bkBpJ2AXatPjmr/+Q2snOIcK1P+ZMiOSyiRfdoK1iIiIiLiHX4BTu91TKuTfdqJ67K1j2SbT9u/zKnzace0gcAQd+s/TwrWIiIiIlIwjIEKdZ1bZp920k7YuuTkNH9Zfdq+OcynXbz6tNVjLSIiIiLuyerT9oxqZ+/Tjqx5snUkLgGiahWJPm31WIuIiIhI0XOmPu3Maf7+mJFzn3a1BKgSDz6+rpV+OgVrERERESk6svdpt7/d06f956nT/K39Enz84YGt4BPsdsVZFKxFREREpOgyBirUc24thzn7knbA3rXgX3RCNShYi4iIiEhxE1bVuRUxPm4XICIiIiJSEihYi4iIiIh4gYK1iIiIiIgXKFiLiIiIiHiBgrWIiIiIiBcoWIuIiIiIeIGCtYiIiIiIFyhYi4iIiIh4gYK1iIiIiIgXKFiLiIiIiHiBgrWIiIiIiBcoWIuIiIiIeIGCtYiIiIiIFyhYi4iIiIh4gYK1iIiIiIgXGGut2zV4hTFmL7DZ7TpECkB5INHtIqTE0udLCpI+X1KQ3Px8VbPWVjh9Z4kJ1iIllTFmmbW2ldt1SMmkz5cUJH2+pCAVxc+XWkFERERERLxAwVpERERExAsUrEWKvoluFyAlmj5fUpD0+ZKCVOQ+X+qxFhERERHxAo1Yi4iIiIh4gYK1iIiIiIgXKFiLuMwY85wxZoEx5oXT9j9sjFnsuXX37BtmjPnDGDPXGPOUOxVLcXWWz9pYY8wqz+fqbrfqk+LpLJ+r5z2fqbnGmAOeffoZJufEGFPVGLPCGJNijPHL4dhsY8wPxpgebtUICtYirjLGtADKWms7AgHGmNbZDr9trU0AegMPZ9v/tLW2i7X2/sKsVYq3XD5rAPd4PlfPulCeFFNn+1xZa++01nYB7gK+yvYw/QyTc7Ef6A4syeHYGOBfQE/Pv65RsBZxVwIw07M9E2ibecBau9GzeRzIfpXxncaY+Zmj2CJ5dMbPmseTxpiZxpj4Qq1KirvcPlcAA4BPs93XzzDJN2ttirX2wBkONwUWW2uTgcPGmNBCLO0UCtYi7ioHJHm2DwEROZwzFnjVsz0N5wfIIGCcMca3YMuTEqQcZ/6sjbfWtgRuAl4s5LqkeCtH7j/DegHfeLanoZ9h4n2+9uQ0d2f6HBYKBWsRdx0EwjzbYZ77WYwxA4Aoa+17ANbag9baDGvtXuBPoFLhlSrF3EHO8Fmz1u73/Luu0KuS4u4gZ/8ZVgfYbq09CvoZJgUmPdv23z6HhUnBWsRdi3F6xgB6kK13zBjTFLjFc8vcF+b5NxioA+wttEqluDvbZy3zc1Ue8Pv7Q0XO6IyfK48BwGeZd/QzTArIamNMgjGmLBBmrU3K9REFRMFaxEXW2hVAijFmAZBhrV1qjMn8U/zTOKM53xpjPvfsu8sYsxiYCzxhrU0t9KKlWMrts2aMWQR8gXMRkEie5PK5AuiL87nKpJ9hck6MMf7GmJlAM5z/Fy/I9ll7CngMp8//cbdqBK28KCIiIiLiFRqxFhERERHxAgVrEREREREvULAWEREREfECBWsRERERES9QsBYRERER8QIFaxGRfDLGTDbGfHmm+2d53FhjzKSCre6U15trjHkpl3O6GGOsZw7rIiuv73EenscaYwZ7o6ZszxlojNlijGnlzecVkeJHwVpEig1PuLKeW5onzLxijHFt+dq8MsZUBO4GHs22L/vXk2qM2WCMGedZ5MAbBgIPZHu9TcaYe0875wegCrDPS6+ZozO8drFgjBlojPnWGLPX873qkv24tfY4zrzzT7pRn4gUHQrWIlLczMQJgtWBG4F+wP/cLCiPbgSWWms3nLY/8+upCfwLuBkY540XtNbut9YezuWcE9baXVaLGpxNWZxfQO4+yzlTgA7GmEaFU5KIFEUK1iJS3Bz3BMFt1trvgA+BntlPMMYMN8b8boxJMcb8aYy5yxjjk+14mGeke6fnnDXGmKGeY1HGmPeNMduMMceMMb8ZY4Z7oe4rgeln+Xq2Wmvfwwlol3pqCTTGPG+M2e2pc4kxpkO2r8PfGDPeGLPDGHPcGLPVGPNEtuNZrSDGmLlANZxVFq0xxnr2Z7WCGGPCPV9zv9Pez56eEfWKnvvRxpgPjDEHPLevjDF1zvWNMcb4GmPeMMZs9Lz+OmPM/dm/Z9nO/Zfn/Ug2xrzpWRo785jxPG6953l+McZcfa51ZbLWvmOtfQSYcZZz9gOLgCvO9/VEpPjyc7sAEZFzZYypCfQCUrPtGwH8H3AbsBxoDLzmOeclY4zBCUgRwHDgT6AeEOR5iiBgBc6f9ZOAHsCrxpgt1tpZ51hnJNAQWJaH048B/p7tp4DLgOuBDTgjpt8YY+pYa3cCtwMDgMuBTUCM52vJyUBgFTAJeCWnE6y1hzx9zFdx6jLUVwHfWWv3GGPKAHNwRnA7AyeAe4GZxpgG1tqjefgaT+cDbPd8rXuBNsBEnPaUN7Kd1xnn/ekORHu+lidx3gdw2mwGA7cAfwAJwGvGmAPW2q9yemHPLxxYa7ucQ92nW+qpUURKKQVrESluehljkgFfTobh7H+i/zdwv7X2Y8/9jZ5R3JuBl3CCcgLQyFq7xnNOVnuGtXY7Tr9sponGmG44I5HnFKyBOMAAO892kjGmDc7I9ixPn/VNwI2ZodAYMxrohhMc/4UzAv0nsMDTyrEFJ/D+jbV2vzEmHThsrd11ljLeBd43xoRaaw97RoQHAKM8xy/3fC3DM9tHjDGjgD1AX+Cjs74TOdeWCjyUbdcmY0wLnPc8e7BO97xuMvCrMeYfwBvGmMw+8ruBntbaBZ77Gz3v6S1AjsEa5z3zlh04LUoiUkopWItIcTMfGAkEAyOAWsB4AGNMBSAWZ4Q5+6isH04YBGgO7MwWqk9hjPEFxgBDcUZFA4EAYO551JzZrpCSw7HMXxT8cEaqP8cZba/lub8o80RrbboxZjHO6DfAZOB74E9jzHfA18AMa23GedT6NXAUJ0y/DfTHee8+9xxvCdQADjuD/1nKeGo+J55fGm7E+WUhGOdr33zaaas9oTrTYpzvTS2c71MQzoh+9n5xf5zR/BxZa68915pzcIyT32sRKYUUrEWkuDlqrf3Ls327MWYOzij1WE5eNzKaM4zccjJgn8m9wD3AHcAvQDLwOFDxPGpO9Pwbwd9HrTN/UUgFdnhGbzHGVPEcz+miQgtgrV1hjKmO0w7TDXgLWGWMufBcw7W1NtUYMxWn/eNtz7+fZmvx8AFW4oxcn27/ubymp7/9eZz3/gecFpxbcMJ9XmV+7/vx91HoVApHJE4ri4iUUgrWIlLcPQLMMMZMtNbuMMZsB2pZa98+w/krgCqefuCcRq07AF9Ya98B54I4oC5w8DxqXI8TFhsCv592LPsvCtn9hdO/3AFPq4pnND0BeC/zJM+sH1OBqcaYycASoDZOi8jpTuC00OTmXWCeMaYhTmjvk+3YCpwWjURr7cE8PFdedAB+tNZmzbltjMlp9LuJMaastfaI535bnK9pPU6wPg5Us9bO9lJd+dUY5/0RkVJKwVpEijVr7VxjzG+cnKpuLPCiMeYgTluDP9ACiLbW/henT/pH4BNjzF04AbQ2UNZaO81zf6hn9o1EnLaMGsDP51FjhjFmJk6A/Di38z2POeJpZ3nCGJMIbATuAirhmV7QGHM3zgj4SpxR2StxAvy2MzztJqCjMeZdnNlIEnM6yVq7yBizGSfAJwLZg+oUnJHlz40xD+GMDscClwATrLXrzvJlVTXGxJ+2bxvOez7MGNMb5xeKy3EuAjxw2rl+wCRjzP8BVYEngNcyg7YxZhwwzvPL0HwgBCd8Z1hrJ+ZUkDHmbc/XfMaWEM/Fp3FAOc+u2p7P167T+tU74vz1RERKKU23JyIlwbPADcaYatba13Fm0bgGZxaMBTitFhvBCblAb5ze5XeBNcALOL264MwssRRn5pD5wBGcMHm+JuIE9ryMGGf6B87FgG/ihOemQC/PjCAAh4H7PPWuAOKB3meZmeMhnBC8ntxbFqYAzYD3rbXpmTs9z90JZxR9KrAWpwUlgr8H4dPdhfMLSvbb5cCrnq/zPeAnnAsAn8nh8fOA33BmJfkMJ/Dfn+14ZkvQvZ7zvgcG4fnen0Gc53Y2/T21zvHcf81zf3TmCcaYBCCcPP7iJCIlk9GaACIihcNz4eH/MttMpOTw9KX/bK193O1aRMQ9GrEWESk8o9DP3RLHGBOI89eR59yuRUTcpRFrEREREREv0MiJiIiIiIgXKFiLiIiIiHiBgrWIiIiIiBcoWIuIiIiIeIGCtYiIiIiIFyhYi4iIiIh4wf8DVs2l+/rin90AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "p = nb.predict_proba(X_test)\n",
    "#check = [i[1] for i in p]\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, p.take(1,1))\n",
    "plt.figure(figsize = (12,7))\n",
    "plt.plot(recall,precision)\n",
    "plt.plot(gary_recall, gary_precision)\n",
    "#plt.plot(tn_recall, tn_precision)\n",
    "plt.xlabel('Recall (Positive Label: 1)', fontsize = 14)\n",
    "plt.ylabel('Precision (Positive Label: 1)', fontsize = 14)\n",
    "ticks = [0.25,0.5,0.75,1.0]\n",
    "plt.xticks(ticks,ticks, fontsize = 8)\n",
    "plt.yticks(ticks,ticks, fontsize = 8)\n",
    "plt.legend(['Naive Bayes', 'Random Forest'])\n",
    "plt.savefig('precision_recall_curve_group.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "899dc827-a9bd-40cb-9f3b-31298a4cdb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13570982, 0.26111111, 1.        ])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gary_precision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
