{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3a12c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages and data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import random\n",
    "\n",
    "random.seed(1337)\n",
    "df = pd.read_csv(\"winequality-red.csv\", delimiter = ';')\n",
    "\n",
    "def qclass(x):\n",
    "    if x >= 7:\n",
    "        return 'above_avg'\n",
    "    else:\n",
    "        return 'avg_or_less'\n",
    "\n",
    "df['quality_class'] = df['quality'].apply(qclass)\n",
    "\n",
    "df = df.drop('quality', axis=1)\n",
    "X = df.drop('quality_class', axis=1)\n",
    "y = df['quality_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "429c72d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#func to partition data at a given boundary\n",
    "def partition(data, feature, boundary):\n",
    "\n",
    "    true_rows = data[data[feature] >= boundary]\n",
    "    false_rows = data[data[feature] < boundary]\n",
    "            \n",
    "    return true_rows, false_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aa7da73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the gini impurity for calculating the impurity of a node\n",
    "def gini(data, target):\n",
    "\n",
    "    counts = data[target].value_counts()\n",
    "    impurity = 1\n",
    "    \n",
    "    for c in counts:\n",
    "        prob_of_label = c/float(len(data))\n",
    "        impurity -= prob_of_label**2\n",
    "        \n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b385cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the function to calculate information gain from splitting a node\n",
    "def info_gain(left, right, current_uncertainty, target):\n",
    "\n",
    "    p = float(len(left))/(len(left)+len(right))\n",
    "    split_uncertainty = p*gini(left, target) + (1-p)*gini(right, target)\n",
    "    ig = current_uncertainty - split_uncertainty\n",
    "    \n",
    "    return ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "339c9773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick the best feature and boundary combination that produces the highest information gain \n",
    "def best_split(data, target):\n",
    "\n",
    "    best_gain = 0\n",
    "    best_feature = None\n",
    "    best_boundary = None\n",
    "    current_uncertainty = gini(data, target)\n",
    "    features = data.columns.drop(target)\n",
    "\n",
    "    for f in features:\n",
    "        if best_feature == None:\n",
    "            best_feature = f\n",
    "            \n",
    "        boundaries = list(set(data[f]))\n",
    "\n",
    "        for b in boundaries:\n",
    "            if best_boundary == None:\n",
    "                best_boundary = b\n",
    "                \n",
    "            tr, fr = partition(data, f, b)\n",
    "            \n",
    "            if (len(tr) == 0) or (len(fr) == 0):\n",
    "                continue\n",
    "\n",
    "            gain = info_gain(tr, fr, current_uncertainty, target)\n",
    "\n",
    "            if gain >= best_gain:\n",
    "                best_gain, best_feature, best_boundary = gain, f, b\n",
    "\n",
    "    return best_gain, best_feature, best_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a605c334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create leaf nodes or branch nodes based on a number of conditions for the current data\n",
    "def make_node(data, target, split_level, min_leaf, max_depth):\n",
    "    \n",
    "    gain, feature, boundary = best_split(data, target)\n",
    "    node = {\n",
    "            'info_gain': gain, \n",
    "            'size': len(data), \n",
    "            'feature': feature, \n",
    "            'boundary': boundary\n",
    "           }\n",
    "    \n",
    "    if (gain == 0) or (len(data) <= min_leaf) or (split_level > max_depth):\n",
    "        node['node_type'] = 'leaf'\n",
    "        prediction = {}\n",
    "        counts = data[target].value_counts()\n",
    "        prediction = 0\n",
    "        prediction_chance = 0\n",
    "        for i in counts.index:\n",
    "            prediction_chance_i = counts[i]/float(len(data))\n",
    "            if prediction_chance_i > prediction_chance:\n",
    "                prediction_chance = prediction_chance_i\n",
    "                prediction = i\n",
    "        node['prediction'] = prediction\n",
    "        node['prediction_chance'] = prediction_chance\n",
    "        \n",
    "    elif gain > 0:\n",
    "        node['node_type'] = 'branch'\n",
    "        \n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a6e44be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the performance metrics of a model for a given class\n",
    "def get_metrics(pred, target='quality_class', positive_class='above_avg'):\n",
    "    \n",
    "    correct_preds = pred[pred[target]==pred['predictions']]\n",
    "    correct = len(correct_preds)\n",
    "    correct_pos_preds = correct_preds[correct_preds[target]==positive_class]\n",
    "    correct_positive = len(correct_pos_preds)\n",
    "    positive_actual = len(pred[pred[target]==positive_class])\n",
    "    positive_pred = len(pred[pred['predictions']==positive_class])\n",
    "    total = len(pred)\n",
    "    acc = correct/total\n",
    "    precision = correct_positive/positive_pred\n",
    "    recall = correct_positive/positive_actual\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "    print('accuracy:',round(acc, 4),', precision:',round(precision, 4),', recall:',round(recall, 4),', f1:',round(f1, 4))\n",
    "    \n",
    "    return acc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1deb1a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the weighted performance metrics of a model\n",
    "def get_weighted_metrics(pred, target='quality_class'):\n",
    "    \n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    f1 = {}\n",
    "    class_n = {}\n",
    "    total = len(pred)\n",
    "    \n",
    "    classes = list(set(pred[target]))\n",
    "    for c in classes:\n",
    "        correct_preds = pred[pred[target]==pred['predictions']]\n",
    "        correct = len(correct_preds)\n",
    "        correct_pos_preds = correct_preds[correct_preds[target]==c]\n",
    "        correct_positive = len(correct_pos_preds)\n",
    "        positive_actual = len(pred[pred[target]==c])\n",
    "        positive_pred = len(pred[pred['predictions']==c])\n",
    "        acc = correct/total\n",
    "        precision[c] = correct_positive/positive_pred\n",
    "        recall[c] = correct_positive/positive_actual\n",
    "        f1[c] = 2*precision[c]*recall[c]/(precision[c]+recall[c])\n",
    "        class_n[c] = positive_actual\n",
    "    \n",
    "    w_precision = 0\n",
    "    w_recall = 0\n",
    "    w_f1 = 0\n",
    "    for c in classes:\n",
    "        w_precision+=(precision[c]*class_n[c]/total)\n",
    "        w_recall+=(recall[c]*class_n[c]/total)\n",
    "        w_f1+=(f1[c]*class_n[c]/total)\n",
    "        \n",
    "    print('accuracy:',round(acc, 4),', wprecision:',round(w_precision, 4),', wrecall:',round(w_recall, 4),', wf1:',round(w_f1, 4))\n",
    "    \n",
    "    return acc, w_precision, w_recall, w_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e074f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is where the magic happens! func to train the decision tree\n",
    "def dtree(data = {}, decision_tree = {}, \n",
    "          split_level = 0, max_depth = 7, min_leaf = 5, \n",
    "          target = 'quality_class'):\n",
    "    \n",
    "    if split_level == 0:\n",
    "        root = make_node(data, target, split_level, min_leaf, max_depth)\n",
    "        root['node_type'] = 'root'\n",
    "        decision_tree[split_level] = [(data, root)]\n",
    "        \n",
    "    decision_tree[split_level + 1] = []\n",
    "    keep_splitting = 0\n",
    "    \n",
    "    \n",
    "    for d, n in decision_tree[split_level]:\n",
    "        \n",
    "        if (n['node_type'] != 'leaf'):\n",
    "            \n",
    "            gain, feature, boundary = best_split(d, target)\n",
    "            tr, fr = partition(d, feature, boundary)\n",
    "\n",
    "            right_branch = make_node(tr, target, split_level + 1, min_leaf, max_depth)\n",
    "            n['right_child_feature'] = right_branch['feature']\n",
    "            n['right_child_boundary'] = right_branch['boundary']\n",
    "            decision_tree[split_level + 1].append((tr, right_branch))\n",
    "            if (right_branch['node_type'] == 'branch'):\n",
    "                keep_splitting = 1\n",
    "\n",
    "            left_branch = make_node(fr, target, split_level + 1, min_leaf, max_depth)\n",
    "            n['left_child_feature'] = left_branch['feature']\n",
    "            n['left_child_boundary'] = left_branch['boundary']\n",
    "            decision_tree[split_level + 1].append((fr, left_branch))\n",
    "            if (left_branch['node_type'] == 'branch'):\n",
    "                keep_splitting = 1\n",
    "    \n",
    "    split_level += 1\n",
    "    \n",
    "    if (keep_splitting == 1):\n",
    "        decision_tree, split_level = dtree(decision_tree = decision_tree, split_level = split_level)\n",
    "        \n",
    "    return decision_tree, split_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "863de709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#func to make predictions based on a trained tree\n",
    "def dtree_predict(data, tree, target = 'quality_class'):\n",
    "\n",
    "    predictions = {}\n",
    "            \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        x = data.iloc[i]\n",
    "        \n",
    "        for s in range(len(tree)):\n",
    "            \n",
    "            if (s == 0):\n",
    "                node = tree[s][0][1]\n",
    "                current_feature = node['feature']\n",
    "                current_boundary = node['boundary']\n",
    "                \n",
    "            for n in range(len(tree[s])):\n",
    "                \n",
    "                node = tree[s][n][1]\n",
    "                \n",
    "                if ((node['feature'] == current_feature) and (node['boundary'] == current_boundary)):\n",
    "                    \n",
    "                    if (node['node_type']!='leaf'):  \n",
    "                        \n",
    "                        if (x[current_feature] >= current_boundary):\n",
    "                            next_feature = node['right_child_feature']\n",
    "                            next_boundary = node['right_child_boundary']\n",
    "\n",
    "                        elif (x[current_feature] < current_boundary):\n",
    "                            next_feature = node['left_child_feature']\n",
    "                            next_boundary = node['left_child_boundary']\n",
    "                            \n",
    "                    if (node['node_type']=='leaf'):\n",
    "                        prediction = node['prediction']\n",
    "        \n",
    "            current_feature = next_feature\n",
    "            current_boundary = next_boundary\n",
    "        \n",
    "        predictions[i] = prediction\n",
    "        \n",
    "    data['predictions'] = pd.Series(predictions)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f978d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest func built upon the previous decision tree func\n",
    "def rforest_predict(test_data, train_data, target = 'quality_class', n_trees = 69):\n",
    "\n",
    "    samples = []\n",
    "    trees = []\n",
    "    dt_predictions = pd.DataFrame()\n",
    "    rf_predictions = []\n",
    "    classes = list(set(test_data[target]))\n",
    "\n",
    "    for i in range(n_trees):\n",
    "        sample_size = int(len(train_data)/2)\n",
    "        sample = train_data.sample(n=sample_size)\n",
    "        sample = sample.reset_index(drop=True)\n",
    "        samples.append(sample)\n",
    "\n",
    "    for s in samples:\n",
    "        tree, sl = dtree(s, target=target)\n",
    "        print('tree completed')\n",
    "        trees.append(tree)\n",
    "\n",
    "    pred_number = 1\n",
    "    for t in trees:\n",
    "        prediction = dtree_predict(test_data, t, target)\n",
    "        dt_predictions['prediction'+str(pred_number)] = prediction['predictions']\n",
    "        pred_number+=1\n",
    "\n",
    "    for i in range(len(dt_predictions)):\n",
    "        counts = {}\n",
    "\n",
    "        for c in classes:\n",
    "            counts[c] = 0\n",
    "\n",
    "        for col in dt_predictions.columns:\n",
    "            dt_pred = dt_predictions[col][i]\n",
    "            counts[dt_pred]+=1\n",
    "\n",
    "        biggest_count = 0\n",
    "        for c in counts.keys():\n",
    "            if counts[c] >= biggest_count:\n",
    "                biggest_count = counts[c]\n",
    "                rf_pred = c\n",
    "\n",
    "        rf_predictions.append(rf_pred)\n",
    "\n",
    "    test_data['rf_predictions'] = pd.Series(rf_predictions)  \n",
    "    \n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4e40361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a dict where keys are a fold and values are lists containing training and testing data,\n",
    "#training data for each fold has been undersampled to address class imbalance as well as reduce training runtime\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "skf.get_n_splits(X.copy(), y.copy())\n",
    "\n",
    "fold = 1\n",
    "skf_df ={}\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    \n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    xtr, ytr = rus.fit_resample(X.iloc[train_index], y.iloc[train_index])\n",
    "    train_df = pd.concat([xtr, ytr], axis=1).reset_index(drop=True)\n",
    "    test_df = pd.concat([X.iloc[test_index], y.iloc[test_index]], axis=1).reset_index(drop=True)\n",
    "    skf_df[fold] = [train_df, test_df]\n",
    "    fold+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fc272c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree completed\n",
      "tree completed\n"
     ]
    }
   ],
   "source": [
    "#cross validate model and return performance metrics for each fold\n",
    "rf_cv_results_minority = []\n",
    "rf_cv_results_weighted = []\n",
    "rfpredslist = []\n",
    "\n",
    "for fold in skf_df:\n",
    "    kf_train_df = skf_df[fold][0]\n",
    "    kf_test_df = skf_df[fold][1]\n",
    "    \n",
    "    rfpreds = rforest_predict(test_data=kf_test_df, train_data=kf_train_df, target='quality_class', n_trees=11)\n",
    "    rfpredslist.append(rfpreds)\n",
    "    \n",
    "    fold_results_minority = [get_metrics(pred=rfpreds, target='quality_class', positive_class='above_avg')]\n",
    "    rf_cv_results_minority.append(fold_results_minority)\n",
    "    \n",
    "    fold_results_weighted = [get_weighted_metrics(pred=rfpreds, target='quality_class')]\n",
    "    rf_cv_results_weighted.append(fold_results_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44136863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find mean and std for each performance metric across k folds\n",
    "import numpy as np\n",
    "\n",
    "a = rf_cv_results_minority\n",
    "b = np.asarray(a)\n",
    "c = np.mean(b, axis=1)\n",
    "d = pd.DataFrame(c)\n",
    "e = d.rename(columns = {0:'accuracy', 1:'precision', 2:'recall', 3:'f1'})\n",
    "avg_metrics = e.mean(axis=0)\n",
    "avg_metrics.name = 'mean_scores'\n",
    "std_metrics = e.std(axis=0)\n",
    "std_metrics.name = 'std_of_scores'\n",
    "\n",
    "print(avg_metrics)\n",
    "print(std_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74034b92",
   "metadata": {},
   "source": [
    "##RESULTS OF PREVIOUS RUN##\n",
    "\n",
    "accuracy     0.684642\n",
    "precision    0.281667\n",
    "recall       0.659524\n",
    "f1           0.378611\n",
    "Name: mean_scores, dtype: float64\n",
    "\n",
    "accuracy     0.138242\n",
    "precision    0.112642\n",
    "recall       0.203107\n",
    "f1           0.120722\n",
    "Name: std_of_scores, dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6171e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix and classification report generated for reporting\n",
    "#also, prediction and target data are binarized in preparation for PR curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "bin_labels = {'above_avg': 1, 'avg_or_less': 0}\n",
    "\n",
    "allrfpred = pd.concat(rfpredslist, ignore_index=True)\n",
    "\n",
    "print(confusion_matrix(allrfpred['quality_class'], allrfpred['predictions']))\n",
    "print(classification_report(allrfpred['quality_class'], allrfpred['predictions']))\n",
    "\n",
    "allrfpred = allrfpred.replace(bin_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84934c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot PR curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from matplotlib import style\n",
    "\n",
    "#style.use('dark_background')\n",
    "\n",
    "gary_precision, gary_recall, gary_thresholds = precision_recall_curve(allrfpred['quality_class'], allrfpred['predictions'])\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(gary_recall, gary_precision)\n",
    "plt.xlabel('Recall (Positive Label: above_avg)', fontsize=8)\n",
    "plt.ylabel('Precision (Positive Label: above_avg)', fontsize=8)\n",
    "ticks = [.25, .5, .75, 1.00]\n",
    "plt.xticks(ticks, ticks, fontsize=8)\n",
    "plt.yticks(ticks, ticks, fontsize=8)\n",
    "plt.savefig('precision_recall_curve_gary.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
